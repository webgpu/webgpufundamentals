Title: WebGPU í…ìŠ¤ì²˜
Description: í…ìŠ¤ì²˜ ì‚¬ìš©í•˜ê¸°
TOC: í…ìŠ¤ì²˜

ì´ ê¸€ì—ì„œëŠ” í…ìŠ¤ì²˜(texture)ì˜ ê¸°ë³¸ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.
ì´ì „ ê¸€ì—ì„œ ìš°ë¦¬ëŠ” ë°ì´í„°ë¥¼ ì…°ì´ë”ì— ì „ë‹¬í•˜ëŠ” ì£¼ìš” ë°©ë²•ë“¤ì„ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤.
ì´ëŠ” [ìŠ¤í…Œì´ì§€ê°„ ë³€ìˆ˜](webgpu-inter-stage-variables.html),
[uniforms](webgpu-uniforms.html), [ìŠ¤í† ë¦¬ì§€ ë²„í¼](webgpu-storage-buffers.html),
[ì •ì  ë²„í¼](webgpu-vertex-buffers)ì˜€ìŠµë‹ˆë‹¤.
ì…°ì´ë”ì— ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ëŠ” ë§ˆì§€ë§‰ ì£¼ìš” ë°©ë²•ì€ í…ìŠ¤ì²˜ì…ë‹ˆë‹¤.

í…ìŠ¤ì²˜ëŠ” ì£¼ë¡œ 2ì°¨ì› ì´ë¯¸ì§€ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.
2ì°¨ì› ì´ë¯¸ì§€ëŠ” ìƒ‰ìƒê°’ì˜ 2ì°¨ì› ë°°ì—´ì¼ ë¿ì´ë¼ëŠ” ê²ƒì„ ìƒê°í•´ë³´ë©´ ì™œ 2ì°¨ì› ë°°ì—´ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ê¸° ìœ„í•´ í…ìŠ¤ì²˜ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ëŠ”ì§€ ì˜ë¬¸ì´ ìƒê¸°ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ê·¸ëƒ¥ ìŠ¤í† ë¦¬ì§€ ë²„í¼ë¥¼ 2ì°¨ì› ë°°ì—´ë¡œ ë§Œë“¤ì–´ë„ ë˜ì£ .
í…ìŠ¤ì²˜ê°€ íŠ¹ë³„í•œ ì´ìœ ëŠ” *ìƒ˜í”ŒëŸ¬(sampler)*ë¼ëŠ” íŠ¹ìˆ˜í•œ í•˜ë“œì›¨ì–´ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. 
ìƒ˜í”ŒëŸ¬ëŠ” í…ìŠ¤ì²˜ë¡œë¶€í„° 16ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ê°’ì„ ì½ì„ ìˆ˜ ìˆê³ , ì´ë“¤ì„ ë‹¤ì–‘í•œ ì‚¬ìš© ìš©ë„ì— ë§ê²Œ ì ì ˆíˆ ì„ì„ ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

í•˜ë‚˜ì˜ ì˜ˆì‹œë¡œ, 2ì°¨ì› ì´ë¯¸ì§€ë¥¼ ì›ë˜ í¬ê¸°ë³´ë‹¤ ë” í¬ê²Œ ê·¸ë¦¬ê³  ì‹¶ë‹¤ê³  í•´ ë´…ì‹œë‹¤.

<div class="center">
  <div>
    <div><img class="pixel-perfect" src="resources/kiana.png" style="max-width: 100%; width: 128px; height: 128px; image-rendering: pixelated; image-rendering: crisp-edges;"></div>
    <div style="text-align: center;">ì›ë³¸</div>
  </div>
</div>

ë‹¨ìˆœíˆ ì›ë³¸ ì´ë¯¸ì§€ë¡œë¶€í„° í•˜ë‚˜ì˜ í”½ì…€ì„ ê°€ì ¸ì™€ ê° í”½ì…€ì„ ë” í° ì´ë¯¸ì§€ë¡œ ë§Œë“¤ë©´ ì•„ë˜ ì²« ë²ˆì§¸ ì˜ˆì œê°™ì´ ë³´ì´ê²Œ ë©ë‹ˆë‹¤.
ëŒ€ì‹ ì— í•˜ë‚˜ì˜ í”½ì…€ì„ ê°€ì§€ê³  ë” í° ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ ë•Œ ì›ë³¸ ì´ë¯¸ì§€ì˜ ì—¬ëŸ¬ í”½ì…€ì„ ê³ ë ¤í•´ì„œ ë§Œë“¤ë©´, ì•„ë˜ ì˜¤ë¥¸ìª½ì²˜ëŸ¼ ëœ í”½ì…€í™”(pixelated)ëœ ì´ë¯¸ì§€ë¥¼ ë³¼ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.

<div class="webgpu_center compare">
  <div>
    <div><img class="pixel-perfect" src="resources/kiana.png" style="max-width: 100%; width: 512px; height: 512px; image-rendering: pixelated; image-rendering: crisp-edges;"></div>
    <div>í•„í„°ë§ ë˜ì§€ ì•Šì•˜ì„ ë•Œ</div>
  </div>
  <div>
    <div><img class="pixel-perfect" src="resources/kiana.png" style="max-width: 100%; width: 512px; height: 512px;"></div>
    <div>í•„í„°ë§ ë˜ì—ˆì„ ë•Œ</div>
  </div>
</div>

í…ìŠ¤ì²˜ë¡œë¶€í„° ê°œë³„ì ì¸ í”½ì…€ì„ ì–»ì–´ì˜¤ëŠ” WGSL í•¨ìˆ˜ê°€ ìˆê³ , ì´ë“¤ë„ ì‚¬ìš©ì„ ì•ˆí•˜ëŠ” ê²ƒì€ ì•„ë‹ˆì§€ë§Œ 
ì´ëŸ¬í•œ í•¨ìˆ˜ë“¤ì€ í¥ë¯¸ë¡­ì§€ ì•Šì€ ê²ƒì´, ë™ì¼í•œ ì‘ì—…ì„ ìŠ¤í† ë¦¬ì§€ ë²„í¼ë¡œë„ í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. 
WGSLì˜ í…ìŠ¤ì²˜ ê´€ë ¨í•œ í¥ë¯¸ë¡œìš´ í•¨ìˆ˜ë“¤ì€ ì—¬ëŸ¬ í”½ì…€ë“¤ì„ í•„í„°ë§í•˜ê³  ì„ëŠ” í•¨ìˆ˜ë“¤ì…ë‹ˆë‹¤.

WGSL í•¨ìˆ˜ëŠ” ë°ì´í„°ë¥¼ í‘œí˜„í•˜ëŠ” í…ìŠ¤ì²˜ì™€, í…ìŠ¤ì²˜ë¡œë¶€í„° ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ì–»ì–´ì˜¬ ê²ƒì¸ì§€ë¥¼ í‘œí˜„í•˜ëŠ” ìƒ˜í”ŒëŸ¬, 
ê·¸ë¦¬ê³  ê°’ì„ ì–»ì–´ì˜¤ê³ ì í•˜ëŠ” í…ìŠ¤ì²˜ ì¢Œí‘œë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.

ìƒ˜í”Œë§ëœ í…ìŠ¤ì²˜ì— ëŒ€í•œ í…ìŠ¤ì²˜ ì¢Œí‘œëŠ” ê°€ë¡œì„¸ë¡œ 0.0ì—ì„œ 1.0 ì‚¬ì´ì´ê³  ì´ëŠ” ì‹¤ì œ í…ìŠ¤ì²˜ì˜ í¬ê¸°ì™€ëŠ” ê´€ê³„ ì—†ìŠµë‹ˆë‹¤. [^up-or-down]

[^up-or-down]: í…ìŠ¤ì²˜ ì¢Œí‘œê°€ ìœ„(0 = bottom, 1 = top)ì¸ì§€ ì•„ë˜(0 = top, 1 = bottom)ì¸ì§€ëŠ” ê´€ì ì˜ ì°¨ì´ì…ë‹ˆë‹¤.
ì¤‘ìš”í•œ ê²ƒì€ í…ìŠ¤ì²˜ ì¢Œí‘œ 0,0ì´ í…ìŠ¤ì²˜ì˜ ì²« ë°ì´í„°ë¥¼ ì°¸ì¡°í•œë‹¤ëŠ” ì‚¬ì‹¤ì…ë‹ˆë‹¤.

<div class="webgpu_center"><img src="resources/texture-coordinates-diagram.svg" style="width: 500px;"></div>

[ìŠ¤í…Œì´ì§€ê°„ ë³€ìˆ˜ì— ê´€í•œ ê¸€](webgpu-inter-stage-variables.html)ì˜ ì˜ˆì œë¥¼ ê°€ì§€ê³  
ìˆ˜ì •í•´ì„œ ì‚¬ê°í˜•(ì‚¼ê°í˜• ë‘ ê°œ)ì— í…ìŠ¤ì²˜ë¥¼ ê·¸ë¦¬ë„ë¡ í•´ ë´…ì‹œë‹¤.

```wgsl
struct OurVertexShaderOutput {
  @builtin(position) position: vec4f,
-  @location(0) color: vec4f,
+  @location(0) texcoord: vec2f,
};

@vertex fn vs(
  @builtin(vertex_index) vertexIndex : u32
) -> OurVertexShaderOutput {
-  let pos = array(
-    vec2f( 0.0,  0.5),  // top center
-    vec2f(-0.5, -0.5),  // bottom left
-    vec2f( 0.5, -0.5)   // bottom right
-  );
-  var color = array<vec4f, 3>(
-    vec4f(1, 0, 0, 1), // red
-    vec4f(0, 1, 0, 1), // green
-    vec4f(0, 0, 1, 1), // blue
-  );
+  let pos = array(
+    // 1st triangle
+    vec2f( 0.0,  0.0),  // center
+    vec2f( 1.0,  0.0),  // right, center
+    vec2f( 0.0,  1.0),  // center, top
+
+    // 2st triangle
+    vec2f( 0.0,  1.0),  // center, top
+    vec2f( 1.0,  0.0),  // right, center
+    vec2f( 1.0,  1.0),  // right, top
+  );

  var vsOutput: OurVertexShaderOutput;
-  vsOutput.position = vec4f(pos[vertexIndex], 0.0, 1.0);
-  vsOutput.color = color[vertexIndex];
+  let xy = pos[vertexIndex];
+  vsOutput.position = vec4f(xy, 0.0, 1.0);
+  vsOutput.texcoord = xy;
  return vsOutput;
}

+@group(0) @binding(0) var ourSampler: sampler;
+@group(0) @binding(1) var ourTexture: texture_2d<f32>;

@fragment fn fs(fsInput: OurVertexShaderOutput) -> @location(0) vec4f {
-  return fsInput.color;
+  return textureSample(ourTexture, ourSampler, fsInput.texcoord);
}
```

ìœ„ ì˜ˆì œì—ì„œ ìš°ë¦¬ëŠ” ìº”ë²„ìŠ¤ ì¤‘ì‹¬ì— ì‚¼ê°í˜•ì„ ê·¸ë¦¬ê¸° ìœ„í•œ ì„¸ ê°œì˜ ì •ì ì„ 
ìº”ë²„ìŠ¤ ì˜¤ë¥¸ìª½ ìœ„ì— ì‚¬ê°í˜•ì„ ê·¸ë¦¬ê¸° ìœ„í•œ ì—¬ì„¯ ê°œì˜ ì •ì ìœ¼ë¡œ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.

`OutVertexShaderOutput`ë¥¼ `vec2f`ì¸ `texcoord`ë¥¼ ì „ë‹¬í•˜ë„ë¡ ìˆ˜ì •í•˜ì˜€ê³ , ì´ë¥¼ í†µí•´ í…ìŠ¤ì²˜ ì¢Œí‘œë¥¼ í”„ë˜ê·¸ë¨¼íŠ¸ ì…°ì´ë”ë¡œ ë„˜ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì •ì  ì…°ì´ë”ì—ì„œ `vsOutput.texcoord`ë¥¼ í´ë¦½ ê³µê°„ ìœ„ì¹˜ì™€ ê°™ì€ ê°’ìœ¼ë¡œ ì„¤ì •í•˜ì˜€ê³ , 
ì´ëŠ” í•˜ë“œì½”ë”©ëœ ìœ„ì¹˜ê°’ê³¼ ê°™ì€ ê°’ì…ë‹ˆë‹¤. 
`vsOutput.texcoord`ëŠ” í”„ë˜ê·¸ë¨¼íŠ¸ ì…°ì´ë”ë¡œ ë„˜ì–´ê°€ë©´ì„œ ì‚¼ê°í˜•ì˜ ì„¸ ê°œ ì •ì  ì‚¬ì´ì—ì„œ ë³´ê°„ë©ë‹ˆë‹¤.

ê·¸ë¦¬ê³  ìƒ˜í”ŒëŸ¬ì™€ í…ìŠ¤ì²˜ë¥¼ ì„ ì–¸í•˜ê³  í”„ë˜ê·¸ë¨¼íŠ¸ ì…°ì´ë”ì—ì„œ ì´ë“¤ì„ ì°¸ì¡°í•©ë‹ˆë‹¤.
`textureSample`í•¨ìˆ˜ëŠ” í…ìŠ¤ì²˜ë¥¼ *ìƒ˜í”Œë§*í•©ë‹ˆë‹¤. 
ì²« ë²ˆì§¸ ì¸ìëŠ” ìƒ˜í”Œë§í•  í…ìŠ¤ì²˜ì´ê³ , ë‘ ë²ˆì§¸ ì¸ìëŠ” í…ìŠ¤ì²˜ë¥¼ ìƒ˜í”Œë§í•œ ë°©ë²•ì´ ëª…ì‹œëœ ìƒ˜í”ŒëŸ¬ì´ë©° 
ì„¸ ë²ˆì§¸ ì¸ìëŠ” ì–´ë””ì„œ ìƒ˜í”Œë§í•  ê²ƒì¸ì§€ì— ëŒ€í•œ í…ìŠ¤ì²˜ ì¢Œí‘œì…ë‹ˆë‹¤.

> Note: í…ìŠ¤ì²˜ ì¢Œí‘œë¡œ ìœ„ì¹˜ê°’ì„ ë„˜ê¸°ëŠ” ê²ƒì€ í”í•œ ì¼ì´ ì•„ë‹™ë‹ˆë‹¤.
> ì´ ì˜ˆì œì™€ ê°™ì€ ë‹¨ìœ„ ì‚¬ê°í˜• (ë„ˆë¹„ì™€ ë†’ì´ê°€ 1ì¸ ì‚¬ê°í˜•)ì—ì„œëŠ” 
> ìš°ì—°íˆ ìœ„ì¹˜ê°’ê³¼ í…ìŠ¤ì²˜ ì¢Œí‘œê°€ ì¼ì¹˜í•œ ê²ƒ ë¿ì…ë‹ˆë‹¤.
> ì´ëŸ° ë°©ì‹ìœ¼ë¡œ ìš°ë¦¬ ì˜ˆì œê°€ ê°„ê²°í•˜ê³  ë‹¨ìˆœí•´ ì§‘ë‹ˆë‹¤.
> í…ìŠ¤ì²˜ ì¢Œí‘œëŠ” [ì •ì  ë²„í¼](webgpu-vertex-buffers.html)ë¥¼ í†µí•´
> ì „ë‹¬í•˜ëŠ” ê²ƒì´ í›¨ì”¬ ì¼ë°˜ì ì…ë‹ˆë‹¤.

ì´ì œ í…ìŠ¤ì²˜ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤. 5x7 í¬ê¸°ì˜ `F` í…ì…€(texel)ì„ ë§Œë“¤ê² ìŠµë‹ˆë‹¤. [^texel]

[^texel]: í…ì…€ì€ "texture element"ì˜ ì•½ì–´ë¡œ í”½ì…€ì´ "picture element"ì˜ ì•½ì–´ë‹Œ ê²ƒê³¼ ëŒ€ì‘ë©ë‹ˆë‹¤.
ì €ëŠ” í”½ì…€ì´ë‚˜ í…ì…€ì´ë‚˜ ë™ì¼í•˜ë‹¤ê³  ìƒê°í•˜ì§€ë§Œ ì–´ë–¤ ì‚¬ëŒë“¤ì€ í…ìŠ¤ì²˜ì— ëŒ€í•´ ì´ì•¼ê¸° í•  ë•Œ *í…ì…€*ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ë” ì„ í˜¸í•©ë‹ˆë‹¤.

```js
  const kTextureWidth = 5;
  const kTextureHeight = 7;
  const _ = [255,   0,   0, 255];  // red
  const y = [255, 255,   0, 255];  // yellow
  const b = [  0,   0, 255, 255];  // blue
  const textureData = new Uint8Array([
    b, _, _, _, _,
    _, y, y, y, _,
    _, y, _, _, _,
    _, y, y, _, _,
    _, y, _, _, _,
    _, y, _, _, _,
    _, _, _, _, _,
  ].flat());
```

`F`ê°€ ë³´ì´ì‹¤ ê²ƒì´ê³ , ì™¼ìª½ ìœ„ ì½”ë„ˆ(ì²« ë²ˆì§¸ ê°’)ì—ëŠ” íŒŒë€ìƒ‰ í…ì…€ì´ ìˆìŠµë‹ˆë‹¤.

ìš°ë¦¬ëŠ” `rgba8unorm` í…ìŠ¤ì²˜ë¥¼ ë§Œë“¤ ê²ƒì…ë‹ˆë‹¤. 
`rgba8unorm`ëŠ” í…ìŠ¤ì²˜ê°€ ë¹¨ê°•, ì´ˆë¡, íŒŒë‘ìƒ‰ê³¼ ì•ŒíŒŒ(alpha)ê°’ì„ ê°€ì§ˆ ê²ƒì´ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. 
ê° ê°’ì€ 8ë¹„íŠ¸ ë¶€í˜¸ì—†ëŠ” ê°’ì´ê³  í…ìŠ¤ì²˜ì— ì‚¬ìš©ë  ë–„ ì •ê·œí™”ë  ê²ƒì…ë‹ˆë‹¤.
`unorm`ì€ `unsigned normalzed`ë¼ëŠ” ëœ»ì¸ë° ì´ ê°’ì´ 0ì—ì„œ 255 ì‚¬ì´ì˜ 
ê°’ì„ ê°–ëŠ” ë¶€í˜¸ì—†ëŠ” ë°”ì´íŠ¸ì—ì„œ 0.0ê³¼ 1.0 ì‚¬ì´ì˜ ë¶€ë™ì†Œìˆ˜ì ìœ¼ë¡œ ë³€í™˜ëœ ê²ƒì„ì„ 
ì´ì•¼ê¸°í•˜ëŠ” ë©‹ìˆëŠ” ë‹¨ì–´ì…ë‹ˆë‹¤.

ë‹¤ì‹œ ë§í•´ ìš°ë¦¬ê°€ í…ìŠ¤ì²˜ì— ë„£ì€ ê°’ì´ `[64, 128, 192, 255]`ë¼ë©´ ì…°ì´ë”ì—ì„œëŠ” `[64 / 255, 128 / 255, 192 / 255, 255 / 255]`ê°€ ë˜ê³ , 
ì´ëŠ” ë‹¤ì‹œë§í•´ `[0.25, 0.50, 0.75, 1.00]` ì…ë‹ˆë‹¤.

ì´ì œ ë°ì´í„°ê°€ ì¤€ë¹„ë˜ì—ˆìœ¼ë‹ˆ í…ìŠ¤ì²˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.

```js
  const texture = device.createTexture({
    size: [kTextureWidth, kTextureHeight],
    format: 'rgba8unorm',
    usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST,
  });
```

`device.createTexture`ì—ì„œ `size` ë§¤ê°œë³€ìˆ˜ëŠ” ì´ë¦„ ê·¸ëŒ€ë¡œì£ .
í¬ë§·ì€ ìœ„ì—ì„œ ì´ì•¼ê¸°í•œëŒ€ë¡œ `rgba8unorm`ì´ê³ ìš”. 
`usage`ì˜ `GPUTextureUsage.TEXTURE_BINDING`ëŠ” ìš°ë¦¬ê°€ ì´ í…ìŠ¤ì²˜ë¥¼ ë°”ì¸ë“œê·¸ë£¹[^texture-binding]ì— ë°”ì¸ë”©í•  ê²ƒì„ì„ ì˜ë¯¸í•˜ê³ , 
`COPY_DST`ëŠ” ë°ì´í„°ë¥¼ ë³µì‚¬í•  ìˆ˜ ìˆë„ë¡ í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.

[^texture-binding]: í…ìŠ¤ì²˜ì˜ ë‹¤ë¥¸ ì‚¬ìš© ìš©ë„ ì¤‘ í•˜ë‚˜ëŠ” `GPUTextureUsage.RENDER_ATTACHMENT` ì…ë‹ˆë‹¤.
ì´ëŠ” í…ìŠ¤ì²˜ë¥¼ ìš°ë¦¬ê°€ ë Œë”ë§ì„ í•˜ëŠ” ëŒ€ìƒìœ¼ë¡œ ì“°ê² ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. 
ì˜ˆì œì—ì„œ `context.getCurrentTexture()`ë¥¼ í†µí•´ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” ìº”ë²„ìŠ¤ì˜ í…ìŠ¤ì²˜ëŠ” 
`GPUTextureUsage.RENDER_ATTACHMENT`ê°€ ê¸°ë³¸ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒìœ¼ë¡œ í•  ì¼ì€ ë°ì´í„°ë¥¼ ë³µì‚¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

```js
  device.queue.writeTexture(
      { texture },
      textureData,
      { bytesPerRow: kTextureWidth * 4 },
      { width: kTextureWidth, height: kTextureHeight },
  );
```

`device.queue.writeTexture`ì˜ ì²« ë²ˆì§¸ ë§¤ê°œë³€ìˆ˜ëŠ” ì—…ë°ì´íŠ¸í•˜ê³ ì í•˜ëŠ” í…ìŠ¤ì²˜ì…ë‹ˆë‹¤. 
ë‘ ë²ˆì§¸ëŠ” ë³µì‚¬í•˜ê³ ì í•˜ëŠ” ë°ì´í„°, ì„¸ ë²ˆì§¸ëŠ” í…ìŠ¤ì²˜ì— ë³µì‚¬í•  ë•Œ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ì½ì„ì§€ë¥¼ ëª…ì‹œí•©ë‹ˆë‹¤. 
`bytesPerRow`ê°€ í•œ í–‰(row)ì—ì„œ ë‹¤ìŒ í–‰ìœ¼ë¡œ ë„˜ì–´ê°ˆë•Œê¹Œì§€ ì–¼ë§ˆë‚˜ ë§ì€ ë°”ì´íŠ¸ê°€ ì‚¬ìš©ë˜ëŠ”ì§€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. 
ë§ˆì§€ë§‰ ë§¤ê°œë³€ìˆ˜ëŠ” ë³µì‚¬ ëŒ€ìƒì˜ í¬ê¸°ì…ë‹ˆë‹¤.

ì¶”ê°€ì ìœ¼ë¡œ ìƒ˜í”ŒëŸ¬ë¥¼ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤.

```js
  const sampler = device.createSampler();
```

í…ìŠ¤ì²˜í™” ìƒ˜í”ŒëŸ¬ë¥¼ ëª¨ë‘ ë°”ì¸ë“œê·¸ë£¹ì— ì¶”ê°€í•˜ê³  ì´ëŠ” ìš°ë¦¬ê°€ ì…°ì´ë”ì— ì¶”ê°€í•œ 
`@binding(?)`ì™€ ë§¤ì¹­ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

```js
  const bindGroup = device.createBindGroup({
    layout: pipeline.getBindGroupLayout(0),
    entries: [
      { binding: 0, resource: sampler },
      { binding: 1, resource: texture.createView() },
    ],
  });
```

ë Œë”ë§ ë¶€ë¶„ì—ì„œëŠ” ë°”ì¸ë“œê·¸ë£¹ì„ ëª…ì‹œí•˜ê³  ë‘ ê°œì˜ ì‚¼ê°í˜•ìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì‚¬ê°í˜•ì„ 
ë Œë”ë§í•˜ê¸°ìœ„í•´ ì—¬ì„¯ ê°œì˜ ì •ì ì„ ê·¸ë ¤ì•¼ í•©ë‹ˆë‹¤.

```js
    const pass = encoder.beginRenderPass(renderPassDescriptor);
    pass.setPipeline(pipeline);
+    pass.setBindGroup(0, bindGroup);
-    pass.draw(3);  // call our vertex shader 3 times
+    pass.draw(6);  // call our vertex shader 6 times
    pass.end();
```

ì‹¤í–‰í•˜ë©´ ì•„ë˜ì™€ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ê²Œë©ë‹ˆë‹¤.

{{{example url="../webgpu-simple-textured-quad.html"}}}

**ì™œ Fê°€ ë’¤ì§‘í˜€ìˆì„ê¹Œ?**

ìœ„ë¡œ ë‹¤ì‹œ ì˜¬ë¼ê°€ í…ìŠ¤ì²˜ ì¢Œí‘œì™€ ê´€ë ¨í•œ ë‹¤ì´ì–´ê·¸ë¨ì„ ì‚´í´ë³´ë©´ 
í…ìŠ¤ì²˜ ì¢Œí‘œ 0,0ì´ í…ìŠ¤ì²˜ì˜ ì²« ë²ˆì§¸ í…ì…€ì„ ì°¸ì¡°í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
ì‚¬ê°í˜•ì˜ ìº”ë²„ìŠ¤ ì¤‘ì‹¬ ë¶€ë¶„ì˜ ìœ„ì¹˜ê°€ 0,0ì´ê³  ê·¸ ê°’ì„ í…ìŠ¤ì²˜ ì¢Œí‘œë¡œ ì‚¬ìš©í•˜ë¯€ë¡œ, 
ë‹¤ì´ì–´ê·¸ë¨ì— ëŒ€ì‘í•´ ë³´ë©´ 0,0ì€ ì²« ë²ˆì§¸ì¸ íŒŒë€ìƒ‰ ê°’ì„ ì°¸ì¡°í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì´ë¥¼ ìˆ˜ì •í•˜ëŠ” ë°©ë²•ì€ ì¼ë°˜ì ìœ¼ë¡œ ë‘ ê°€ì§€ì…ë‹ˆë‹¤.

1. í…ìŠ¤ì²˜ ì¢Œí‘œë¥¼ ë’¤ì§‘ëŠ”ë‹¤(flip).

   ì´ ì˜ˆì œì˜ ê²½ìš° í…ìŠ¤ì²˜ ì¢Œí‘œì˜ ìˆ˜ì •ì€ ì •ì  ì…°ì´ë”ì—ì„œ ìˆ˜ì •í•˜ê±°ë‚˜,
      
   ```wgsl
   -  vsOutput.texcoord = xy;
   +  vsOutput.texcoord = vec2f(xy.x, 1.0 - xy.y);
   ```
   
   í”„ë˜ê·¸ë¨¼íŠ¸ ì…°ì´ë”ì—ì„œ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

   ```wgsl
   -  return textureSample(ourTexture, ourSampler, fsInput.texcoord);
   +  let texcoord = vec2f(fsInput.texcoord.x, 1.0 - fsInput.texcoord.y);
   +  return textureSample(ourTexture, ourSampler, texcoord);
   ```
   
   ë‹¹ì—°íˆ [ì •ì  ë²„í¼](webgpu-vertex-buffers.html), ë˜ëŠ” [ìŠ¤í† ë¦¬ì§€ ë²„í¼](webgpu-storage-buffers.html)ë¥¼ ì‚¬ìš©í•´ í…ìŠ¤ì²˜ ì¢Œí‘œë¥¼ ë„˜ê²¨ì£¼ëŠ” ê²½ìš°, 
   ì´ë¥¼ ì›ë³¸ ë°ì´í„°ì—ì„œ ë’¤ì§‘ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

2. í…ìŠ¤ì²˜ ë°ì´í„°ë¥¼ ë’¤ì§‘ëŠ”ë‹¤.

   ```js
    const textureData = new Uint8Array([
   -   b, _, _, _, _,
   -   _, y, y, y, _,
   -   _, y, _, _, _,
   -   _, y, y, _, _,
   -   _, y, _, _, _,
   -   _, y, _, _, _,
   -   _, _, _, _, _,
   +   _, _, _, _, _,
   +   _, y, _, _, _,
   +   _, y, _, _, _,
   +   _, y, y, _, _,
   +   _, y, _, _, _,
   +   _, y, y, y, _,
   +   b, _, _, _, _,
    ].flat());
   ```

   ë°ì´í„°ë¥¼ ë’¤ì§‘ìœ¼ë©´ ìœ„ì— ìˆëŠ” ê°’ì´ ì•„ë˜ë¡œ ì™€ì„œ, ë°”ê¾¸ê¸° ì „ì˜ ì™¼ìª½ ì•„ë˜ ë°ì´í„°ê°€ 
   ì²« ë²ˆì§¸ ë°ì´í„°, ì¦‰ 0,0 í…ìŠ¤ì²˜ ì¢Œí‘œê°€ ì°¸ì¡°í•˜ëŠ” ë°ì´í„°ê°€ ë©ë‹ˆë‹¤. 
   ì´ê²ƒì´ í…ìŠ¤ì²˜ ì¢Œí‘œë¥¼ ëŒ€ê°œ ì•„ë˜ìª½ì´ 0, ìœ„ìª½ì´ 1ë¡œ ìƒê°í•˜ëŠ” ì´ìœ ì…ë‹ˆë‹¤.
   
   <div class="webgpu_center"><img src="resources/texture-coordinates-y-flipped.svg" style="width: 500px;"></div>

   ë°ì´í„°ë¥¼ ë’¤ì§‘ëŠ” ê²ƒì€ í”í•œ ì¼ì´ë¼ ì´ë¯¸ì§€, ë¹„ë””ì˜¤, ìº”ë²„ìŠ¤ë¡œë¶€í„° ë°ì´í„°ë¥¼ ì½ì–´ ì˜¬ ë•Œ ë°ì´í„°ë¥¼ ë’¤ì§‘ì–´ì£¼ëŠ” ì˜µì…˜ì´ ì¡´ì¬í•˜ê¸°ë„ í•©ë‹ˆë‹¤.

## <a id="a-mag-filter"></a>magFilter

ìœ„ ì˜ˆì œì—ì„œ ìš°ë¦¬ëŠ” ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ìƒ˜í”ŒëŸ¬ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. 
5x7 í¬ê¸°ì˜ í…ìŠ¤ì²˜ë¥¼ ì›ë³¸ 5x7 í…ì…€ í¬ê¸°ë³´ë‹¤ í¬ê²Œ ê·¸ë¦¬ê³  ìˆê¸° ë•Œë¬¸ì— ìƒ˜í”ŒëŸ¬ëŠ” 
`magFilter`, ì¦‰ í…ìŠ¤ì²˜ê°€ í™•ëŒ€(magnifying)ë  ë•Œ ì‚¬ìš©ë˜ëŠ” í•„í„°ë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. 
ì´ë¥¼ `nearest` ì—ì„œ `linear`ë¡œ ë°”ê¾¸ë©´ ë„¤ ê°œ í”½ì…€ ì‚¬ì´ì—ì„œ ì„ í˜•(linear) ë³´ê°„í•©ë‹ˆë‹¤.

<a id="a-linear-interpolation"></a>
<div class="webgpu-center center diagram"><div data-diagram="linear-interpolation" style="display: inline-block; width: 600px;"></div></div>

í…ìŠ¤ì²˜ ì¢Œí‘œëŠ” ì¼ë°˜ì ìœ¼ë¡œ "UV"(you-veeë¡œ ë°œìŒ)ë¡œ ë¶ˆë¦¬ë©°, ë”°ë¼ì„œ ìœ„ ë‹¤ì´ì–´ê·¸ë¨ì—ì„œ 
`uv`ëŠ” í…ìŠ¤ì²˜ ì¢Œí‘œë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì£¼ì–´ì§„ uvì— ëŒ€í•´ ê°€ê¹Œìš´ ë„¤ ê°œ í”½ì…€ì´ ì„ íƒë©ë‹ˆë‹¤. 
`t1`ì€ ì„ íƒëœ ì™¼ìª½ ìœ„ í”½ì…€ì˜ ì¤‘ì‹¬ì—ì„œë¶€í„° `u`ì¢Œí‘œê¹Œì§€ì˜ ìˆ˜í‰ ê±°ë¦¬ ë¹„ìœ¨ì´ë©° 0ì€ `u` 
ê°€ ì™¼ìª½ í”½ì…€ì˜ ì¤‘ì‹¬ì„ ìƒì— ìˆë‹¤ëŠ” ëœ»ì´ê³  1ì€ ì˜¤ë¥¸ìª½ í”½ì…€ì˜ ì¤‘ì‹¬ì„ ìƒì— ìˆë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤. 
`t2`ë„ ë¹„ìŠ·í•œë° ìˆ˜í‰ ê±°ë¦¬ê°€ ì•„ë‹Œ ìˆ˜ì§ ê±°ë¦¬ì…ë‹ˆë‹¤.

`t1`ê°’ì€ ìœ„ìª½ ë‘ ê°œì˜ í”½ì…€ê°’ì„ *mix*í•˜ì—¬ ì¤‘ê°„ ìƒ‰ìƒê°’ì„ ê³„ì‚°í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤. 
*mix*ëŠ” ë‘ ê°’ ì‚¬ì´ë¥¼ ì„ í˜• ë³´ê°„í•˜ë©°, `t1`ì´ 0ì´ë©´ ì²« ë²ˆì§¸ ê°’ì´ ì„ íƒë©ë‹ˆë‹¤. 
`t1`ì´ 1ì´ë©´ ë‘ ë²ˆì§¸ ê°’ì´ ì„ íƒë©ë‹ˆë‹¤. 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ì—ì„œëŠ” ë¹„ìœ¨ì— ë”°ë¼ ì„ì´ê²Œ ë©ë‹ˆë‹¤. 
ì˜ˆë¥¼ë“¤ì–´ 0.3ì¼ ê²½ìš° ì²« ë²ˆì§¸ ê°’ì„ 70%, ë‘ ì „ì§¸ ê°’ì„ 30% ì„ìŠµë‹ˆë‹¤. 
ë¹„ìŠ·í•˜ê²Œ ë‘ ë²ˆì§¸ ì¤‘ê°„ ìƒ‰ìƒë„ ì•„ë˜ ë‘ í”½ì…€ê°’ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤. 
ë§ˆì§€ë§‰ìœ¼ë¡œ, `t2`ë¥¼ ì‚¬ìš©í•´ ì´ ë‘ê°œì˜ ì¤‘ê°„ ìƒ‰ìƒê°’ì„ ë‹¤ì‹œ ì„ìœ¼ë©´ ìµœì¢… ìƒ‰ìƒì´ ë©ë‹ˆë‹¤.

ì¤‘ìš”í•œ ë˜ë‹¤ë¥¸ ì ì€ ë‹¤ì´ì–´ê·¸ë¨ ì•„ë˜ìª½ì— ìˆëŠ” ë‘ ê°œì˜ ìƒ˜í”ŒëŸ¬ ì„¤ì •ì¸ `addressModeU`ì™€ 
`addressModeV`ì…ë‹ˆë‹¤. ì´ ê°’ë“¤ì„ `repeat` ë˜ëŠ” 
`clamp-to-edge`ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [^mirror-repeat]
`repeat`ë¡œ ì„¤ì •í•˜ë©´ í…ìŠ¤ì²˜ ì¢Œí‘œê°€ ëª¨ì„œë¦¬ í”½ì…€ì— ëŒ€í•´ ë°”ê¹¥ìª½ìœ¼ë¡œ ì ˆë°˜ì„ ë„˜ì–´ê°€ê²Œ ë˜ë©´ ë°˜ëŒ€ìª½ì˜ í”½ì…€ë¡œ ë˜ëŒì•„ì™€ ìƒ‰ìƒì„ ì„ìŠµë‹ˆë‹¤. 
`clamp-to-edge`ì¸ ê²½ìš° í…ìŠ¤ì²˜ ì¢Œí‘œê°€ clampë˜ì–´ ëª¨ì„œë¦¬ í”½ì…€ ì ˆë°˜ ë°–ìœ¼ë¡œ ë„˜ì–´ê°€ ê³„ì‚°ë  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 
ì´ë ‡ê²Œ ë˜ë©´ í…ìŠ¤ì²˜ ì¢Œí‘œ ë²”ìœ„ ë°–ì˜ ê°’ì— ëŒ€í•´ì„œëŠ” ëª¨ì„œë¦¬ ìƒ‰ìƒë§Œì´ ë³´ì—¬ì§‘ë‹ˆë‹¤.

[^mirror-repeat]: ì¶”ê°€ì ìœ¼ë¡œ `mirror-repeat` ëª¨ë“œë„ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ í…ìŠ¤ì²˜ê°€ "ğŸŸ¥ğŸŸ©ğŸŸ¦"ë¼ë©´, repeatëŠ” "ğŸŸ¥ğŸŸ©ğŸŸ¦ğŸŸ¥ğŸŸ©ğŸŸ¦ğŸŸ¥ğŸŸ©ğŸŸ¦ğŸŸ¥ğŸŸ©ğŸŸ¦"ì¸ë° mirror-repeatëŠ” "ğŸŸ¥ğŸŸ©ğŸŸ¦ğŸŸ¦ğŸŸ©ğŸŸ¥ğŸŸ¥ğŸŸ©ğŸŸ¦ğŸŸ¦ğŸŸ©ğŸŸ¥"ì…ë‹ˆë‹¤.

ì˜ˆì œë¥¼ ìˆ˜ì •í•˜ì—¬ ì´ëŸ° ëª¨ë“  ì˜µì…˜ì„ ì‚¬ìš©í•´ ì‚¬ê°í˜•ì„ ê·¸ë ¤ë³¼ ìˆ˜ ìˆë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.

ë¨¼ì € ê° ì„¤ì •ê°’ì˜ ì¡°í•©ìœ¼ë¡œ ìƒ˜í”ŒëŸ¬ë“¤ì„ ë§Œë“­ë‹ˆë‹¤. 
ë˜í•œ ì´ ìƒ˜í”ŒëŸ¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°”ì¸ë“œê·¸ë£¹ë„ ë§Œë“­ë‹ˆë‹¤.

```js
+  const bindGroups = [];
+  for (let i = 0; i < 8; ++i) {
-   const sampler = device.createSampler();
+   const sampler = device.createSampler({
+      addressModeU: (i & 1) ? 'repeat' : 'clamp-to-edge',
+      addressModeV: (i & 2) ? 'repeat' : 'clamp-to-edge',
+      magFilter: (i & 4) ? 'linear' : 'nearest',
+    });

    const bindGroup = device.createBindGroup({
      layout: pipeline.getBindGroupLayout(0),
      entries: [
        { binding: 0, resource: sampler },
        { binding: 1, resource: texture.createView() },
      ],
    });
+    bindGroups.push(bindGroup);
+  }
```

ì•„ë˜ê³¼ ê°™ì´ ì„¤ì •ë“¤ì„ ë§Œë“­ë‹ˆë‹¤.

```js
  const settings = {
    addressModeU: 'repeat',
    addressModeV: 'repeat',
    magFilter: 'linear',
  };
```

ê·¸ë¦¬ê³  ë Œë”ë§ ì‹œì— ì„¤ì •ê°’ì„ íƒìƒ‰í•´ ì–´ë–¤ ë°”ì¸ë“œ ê·¸ë£¹ì„ ì‚¬ìš©í• ì§€ ê²°ì •í•©ë‹ˆë‹¤.

```js
  function render() {
+    const ndx = (settings.addressModeU === 'repeat' ? 1 : 0) +
+                (settings.addressModeV === 'repeat' ? 2 : 0) +
+                (settings.magFilter === 'linear' ? 4 : 0);
+    const bindGroup = bindGroups[ndx];
   ...
```

ì´ì œ ë‚¨ì€ ê²ƒì€ ì´ëŸ¬í•œ ì„¤ì •ì„ ë°”ê¿€ ìˆ˜ ìˆëŠ” UIë¥¼ ë§Œë“¤ê³  ê°’ì´ ë°”ë€”ë•Œ ë§ˆë‹¤ ë‹¤ì‹œ ë Œë”ë§í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. 
ì €ëŠ” "muigui"ë¼ëŠ”, [dat.GUI](https://github.com/dataarts/dat.gui)ì™€ ìœ ì‚¬í•œ APIë¥¼ ê°–ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

```js
import GUI from '/3rdparty/muigui-0.x.module.js';

...

  const settings = {
    addressModeU: 'repeat',
    addressModeV: 'repeat',
    magFilter: 'linear',
  };

  const addressOptions = ['repeat', 'clamp-to-edge'];
  const filterOptions = ['nearest', 'linear'];

  const gui = new GUI();
  Object.assign(gui.domElement.style, {right: '', left: '15px'});
  gui.add(settings, 'addressModeU', addressOptions).onChange(render);
  gui.add(settings, 'addressModeV', addressOptions).onChange(render);
  gui.add(settings, 'magFilter', filterOptions).onChange(render);
```

ìœ„ ì½”ë“œëŠ” `settings`ë¥¼ ì„ ì–¸í•˜ê³  ì´ë“¤ì„ ì„¤ì •í•˜ëŠ” UIë¥¼ ë§Œë“  í›„, 
ê°’ì´ ë³€ê²½ë˜ëŠ” ê²½ìš°ì— `render`ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.

{{{example url="../webgpu-simple-textured-quad-linear.html"}}}

ìš°ë¦¬ í”„ë˜ê·¸ë¨¼íŠ¸ ì…°ì´ë”ëŠ” ë³´ê°„ëœ í…ìŠ¤ì²˜ ì¢Œí‘œë¥¼ ë°›ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ 
`textureSample`ë¥¼ í˜¸ì¶œí•˜ê¸° ë•Œë¬¸ì— ê° í”½ì…€ì— ëŒ€í•œ ìƒ‰ìƒì„ ìš”ì²­í•  ë•Œ ë‹¤ë¥¸ ì„ì¸ ìƒ‰ìƒì´ ë°˜í™˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
`repeat`ëª¨ë“œì¼ ë•Œ WebGPUê°€ í…ìŠ¤ì²˜ì˜ ë°˜ëŒ€ìª½ì—ì„œ í…ì…€ì„ "ìƒ˜í”Œë§"í•´ ì˜¤ëŠ” ê²ƒì— ì£¼ëª©í•˜ì„¸ìš”.

## <a id="a-min-filter"></a>minFilter

`minFilter` ì„¤ì •ë„ ìˆëŠ”ë° í…ìŠ¤ì²˜ê°€ ì›ë˜ í¬ê¸°ë³´ë‹¤ ì‘ê²Œ ê·¸ë ¤ì§ˆ ë•Œ `magFilter`ì™€ ë¹„ìŠ·í•œ ì—°ì‚°ì„ í•©ë‹ˆë‹¤. 
`linear`ë¡œ ì„¤ì •í•˜ë©´ ë§ˆì°¬ê°€ì§€ë¡œ ë„¤ ê°œì˜ í”½ì…€ì„ ì„ íƒí•˜ê³  ë¹„ìŠ·í•œ ìˆ˜ì‹ì„ í†µí•´ ì„ìŠµë‹ˆë‹¤.

ë¬¸ì œëŠ”, í° í…ìŠ¤ì²˜ë¡œë¶€í„° ë„¤ ê°œì˜ ì„ì„ í”½ì…€ì„ ì„ íƒí•˜ì—¬ ì˜ˆë¥¼ë“¤ì–´ í•˜ë‚˜ì˜ í”½ì…€ 
ìƒ‰ìƒì„ ê²°ì •í•˜ë ¤ê³  í•˜ë©´, ìƒ‰ìƒì´ ë°”ë€Œì–´ ê¹œë°•ì„(flickering) í˜„ìƒì´ ë°œìƒí•˜ê²Œ ë©ë‹ˆë‹¤.

ì§ì ‘ ë§Œë“¤ì–´ì„œ ë¬¸ì œë¥¼ ì‚´í´ ë´…ì‹œë‹¤.

ë¨¼ì € ìº”ë²„ìŠ¤ë¥¼ ì €í•´ìƒë„ë¡œ ë§Œë“­ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ” cssë¥¼ ìˆ˜ì •í•´ì„œ ë¸Œë¼ìš°ì €ê°€ 
ìš°ë¦¬ì˜ ìº”ë²„ìŠ¤ì— ëŒ€í•´ `magFilter: 'linear'`ì™€ ê°™ì€ ì²˜ë¦¬ë¥¼ í•˜ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤. 
ì•„ë˜ì™€ ê°™ì´ cssë¥¼ ì„¤ì •í•˜ë©´ ë©ë‹ˆë‹¤.

```css
canvas {
  display: block;  /* make the canvas act like a block   */
  width: 100%;     /* make the canvas fill its container */
  height: 100%;
+  image-rendering: pixelated;
+  image-rendering: crisp-edges;
}
```

ë‹¤ìŒìœ¼ë¡œ `ResizeObserver` ì½œë°±ì—ì„œ ìº”ë²„ìŠ¤ì˜ í•´ìƒë„ë¥¼ ë‚®ì¶¥ë‹ˆë‹¤.

```js
  const observer = new ResizeObserver(entries => {
    for (const entry of entries) {
      const canvas = entry.target;
-      const width = entry.contentBoxSize[0].inlineSize / 64 | 0;
-      const height = entry.contentBoxSize[0].blockSize / 64 | 0;
+      const width = entry.contentBoxSize[0].inlineSize / 64 | 0;
+      const height = entry.contentBoxSize[0].blockSize / 64 | 0;
      canvas.width = Math.max(1, Math.min(width, device.limits.maxTextureDimension2D));
      canvas.height = Math.max(1, Math.min(height, device.limits.maxTextureDimension2D));
      // re-render
      render();
    }
  });
  observer.observe(canvas);
```

[uniformsì— ê´€í•œ ê¸€](webgpu-uniforms.html)ì˜ ì²« ë²ˆì§¸ ì˜ˆì œì—ì„œì²˜ëŸ¼ 
ì‚¬ê°í˜•ì„ ì˜®ê¸°ê³  í¬ê¸°ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆë„ë¡ í•˜ê¸° ìœ„í•´ uniform ë²„í¼ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.

```wgsl
struct OurVertexShaderOutput {
  @builtin(position) position: vec4f,
  @location(0) texcoord: vec2f,
};

+struct Uniforms {
+  scale: vec2f,
+  offset: vec2f,
+};
+
+@group(0) @binding(2) var<uniform> uni: Uniforms;

@vertex fn vs(
  @builtin(vertex_index) vertexIndex : u32
) -> OurVertexShaderOutput {
  let pos = array(
    // 1st triangle
    vec2f( 0.0,  0.0),  // center
    vec2f( 1.0,  0.0),  // right, center
    vec2f( 0.0,  1.0),  // center, top

    // 2st triangle
    vec2f( 0.0,  1.0),  // center, top
    vec2f( 1.0,  0.0),  // right, center
    vec2f( 1.0,  1.0),  // right, top
  );

  var vsOutput: OurVertexShaderOutput;
  let xy = pos[vertexIndex];
-  vsOutput.position = vec4f(xy, 0.0, 1.0);
+  vsOutput.position = vec4f(xy * uni.scale + uni.offset, 0.0, 1.0);
  vsOutput.texcoord = xy;
  return vsOutput;
}

@group(0) @binding(0) var ourSampler: sampler;
@group(0) @binding(1) var ourTexture: texture_2d<f32>;

@fragment fn fs(fsInput: OurVertexShaderOutput) -> @location(0) vec4f {
  return textureSample(ourTexture, ourSampler, fsInput.texcoord);
}
```

uniformì´ ì¶”ê°€ë˜ì—ˆìœ¼ë‹ˆ uniform ë²„í¼ë¥¼ ë§Œë“¤ê³  ë°”ì¸ë“œ ê·¸ë£¹ì— ì¶”ê°€í•©ë‹ˆë‹¤.

```js
+  // create a buffer for the uniform values
+  const uniformBufferSize =
+    2 * 4 + // scale is 2 32bit floats (4bytes each)
+    2 * 4;  // offset is 2 32bit floats (4bytes each)
+  const uniformBuffer = device.createBuffer({
+    label: 'uniforms for quad',
+    size: uniformBufferSize,
+    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,
+  });
+
+  // create a typedarray to hold the values for the uniforms in JavaScript
+  const uniformValues = new Float32Array(uniformBufferSize / 4);
+
+  // offsets to the various uniform values in float32 indices
+  const kScaleOffset = 0;
+  const kOffsetOffset = 2;

  const bindGroups = [];
  for (let i = 0; i < 8; ++i) {
    const sampler = device.createSampler({
      addressModeU: (i & 1) ? 'repeat' : 'clamp-to-edge',
      addressModeV: (i & 2) ? 'repeat' : 'clamp-to-edge',
      magFilter: (i & 4) ? 'linear' : 'nearest',
    });

    const bindGroup = device.createBindGroup({
      layout: pipeline.getBindGroupLayout(0),
      entries: [
        { binding: 0, resource: sampler },
        { binding: 1, resource: texture.createView() },
+        { binding: 2, resource: { buffer: uniformBuffer }},
      ],
    });
    bindGroups.push(bindGroup);
  }
```

uniformì˜ ê°’ì„ ì„¤ì •í•˜ê³  GPUì— ì—…ë¡œë“œí•˜ëŠ” ì½”ë“œë„ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤. 
ì´ ê³¼ì •ì„ ì• ë‹ˆë©”ì´ì…˜í•  ì˜ˆì •ì´ë¯€ë¡œ `requestAnimationFrame`ë¥¼ ì‚¬ìš©í•˜ë„ë¡ 
ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì—¬ ì—°ì†ì ì¸ ë Œë”ë§ì´ ì´ë£¨ì–´ì§€ë„ë¡ í•©ë‹ˆë‹¤.

```js
  function render(time) {
    time *= 0.001;
    const ndx = (settings.addressModeU === 'repeat' ? 1 : 0) +
                (settings.addressModeV === 'repeat' ? 2 : 0) +
                (settings.magFilter === 'linear' ? 4 : 0);
    const bindGroup = bindGroups[ndx];

+    // compute a scale that will draw our 0 to 1 clip space quad
+    // 2x2 pixels in the canvas.
+    const scaleX = 4 / canvas.width;
+    const scaleY = 4 / canvas.height;
+
+    uniformValues.set([scaleX, scaleY], kScaleOffset); // set the scale
+    uniformValues.set([Math.sin(time * 0.25) * 0.8, -0.8], kOffsetOffset); // set the offset
+
+    // copy the values from JavaScript to the GPU
+    device.queue.writeBuffer(uniformBuffer, 0, uniformValues);

    ...

+    requestAnimationFrame(render);
  }
+  requestAnimationFrame(render);

  const observer = new ResizeObserver(entries => {
    for (const entry of entries) {
      const canvas = entry.target;
      const width = entry.contentBoxSize[0].inlineSize / 64 | 0;
      const height = entry.contentBoxSize[0].blockSize / 64 | 0;
      canvas.width = Math.max(1, Math.min(width, device.limits.maxTextureDimension2D));
      canvas.height = Math.max(1, Math.min(height, device.limits.maxTextureDimension2D));
-      // re-render
-      render();
    }
  });
  observer.observe(canvas);
}
```

The code above sets the scale so that we'll draw the quad the size of 2x2 pixels in the canvas.
It also sets the offset from -0.8 to +0.8 using `Math.sin` so that the quad will
slowly go back and forth across the canvas.

Finally let's add `minFilter` to our settings and combinations

```js
  const bindGroups = [];
  for (let i = 0; i < 16; ++i) {
    const sampler = device.createSampler({
      addressModeU: (i & 1) ? 'repeat' : 'clamp-to-edge',
      addressModeV: (i & 2) ? 'repeat' : 'clamp-to-edge',
      magFilter: (i & 4) ? 'linear' : 'nearest',
+      minFilter: (i & 8) ? 'linear' : 'nearest',
    });

...

  const settings = {
    addressModeU: 'repeat',
    addressModeV: 'repeat',
    magFilter: 'linear',
+    minFilter: 'linear',
  };

  const addressOptions = ['repeat', 'clamp-to-edge'];
  const filterOptions = ['nearest', 'linear'];

  const gui = new GUI();
  Object.assign(gui.domElement.style, {right: '', left: '15px'});
  -gui.add(settings, 'addressModeU', addressOptions).onChange(render);
  -gui.add(settings, 'addressModeV', addressOptions).onChange(render);
  -gui.add(settings, 'magFilter', filterOptions).onChange(render);
+  gui.add(settings, 'addressModeU', addressOptions);
+  gui.add(settings, 'addressModeV', addressOptions);
+  gui.add(settings, 'magFilter', filterOptions);
+  gui.add(settings, 'minFilter', filterOptions);

  function render(time) {
    time *= 0.001;
    const ndx = (settings.addressModeU === 'repeat' ? 1 : 0) +
                (settings.addressModeV === 'repeat' ? 2 : 0) +
-                (settings.magFilter === 'linear' ? 4 : 0);
+                (settings.magFilter === 'linear' ? 4 : 0) +
+                (settings.minFilter === 'linear' ? 8 : 0);
```

We no longer need to call `render` when a setting changes since we're
rendering constantly using `requestAnimationFrame` (often called "rAF"
and this style of rendering loop is often called a "rAF loop")

{{{example url="../webgpu-simple-textured-quad-minfilter.html"}}}

You can see the quad is flickering and changing colors. If the `minFilter`
is set to `nearest` then for each of the 2x2 pixels of the quad it's picking 
one pixel from our texture. If you set it to `linear` then it does the
bilinear filtering we mentioned above but it still flickers.

One reason is, the quad is positioned with real numbers but pixels are integers.
The texture coordinates are interpolated from the real numbers, or rather, they
are computed from the real numbers.

<a id="a-pixel-to-texcoords"></a>
<div class="webgpu-center center diagram">
  <div class="fit-container">
    <div class="text-align: center">drag to move</div>
    <div class="fit-container" data-diagram="pixel-to-texcoords" style="display: inline-block; width: 600px;"></div>
  </div>
</div>

In the diagram above, the <span style="color: red;">red</span> rectangle
represents the quad we asked the GPU to draw based on the values we return
from our vertex shader. When the GPU draws, it computes which pixels' centers
are inside our quad (our 2 triangles). Then, it computes what interpolated
inter-stage variable value to pass to the fragment shader based on where the
center of the pixel to be drawn is relative to the where the original points
are. In our fragment shader we then pass that texture coordinate to the WGSL
`textureSample` function and get back a sampled color as the previous diagram
showed. Hopefully you can see why the colors are flickering. You can see them
blend to different colors depending on which UV coordinates are computed for the
pixel being drawn.

Textures offer a solution to this problem. It's called mip-mapping. I think (but
could be wrong) that "mipmap" stands for "multi-image-pyramid-map".

We take our texture and create a smaller texture that is half the size in each
dimension, rounding down. We then fill the smaller texture with blended colors
from the first original texture. We repeat this until we get to a 1x1 texture.
In our example we have a 5x7 texel texture. Dividing by 2 in each dimension and
rounding down gives us a 2x3 texel texture. We take that one and repeat so we
end up with 1x1 texel texture.

<div class="webgpu-center center diagram"><div data-diagram="mips" style="display: inline-block;"></div></div>

Given a mipmap, we can then ask the GPU to choose a smaller mip level when we're
drawing something smaller than the original texture size. This will look better
because it has been "pre-blended" and better represents what the texture's color
would be when scaled down.

The best algorithm for blending the pixels from one mip to the next is a topic
of research as well as a matter of opinion. As a first idea, here's some code
that generates each mip from the previous mip by bilinear filtering (as
demonstrated above).

```js
const lerp = (a, b, t) => a + (b - a) * t;
const mix = (a, b, t) => a.map((v, i) => lerp(v, b[i], t));
const bilinearFilter = (tl, tr, bl, br, t1, t2) => {
  const t = mix(tl, tr, t1);
  const b = mix(bl, br, t1);
  return mix(t, b, t2);
};

const createNextMipLevelRgba8Unorm = ({data: src, width: srcWidth, height: srcHeight}) => {
  // compute the size of the next mip
  const dstWidth = Math.max(1, srcWidth / 2 | 0);
  const dstHeight = Math.max(1, srcHeight / 2 | 0);
  const dst = new Uint8Array(dstWidth * dstHeight * 4);

  const getSrcPixel = (x, y) => {
    const offset = (y * srcWidth + x) * 4;
    return src.subarray(offset, offset + 4);
  };

  for (let y = 0; y < dstHeight; ++y) {
    for (let x = 0; x < dstWidth; ++x) {
      // compute texcoord of the center of the destination texel
      const u = (x + 0.5) / dstWidth;
      const v = (y + 0.5) / dstHeight;

      // compute the same texcoord in the source - 0.5 a pixel
      const au = (u * srcWidth - 0.5);
      const av = (v * srcHeight - 0.5);

      // compute the src top left texel coord (not texcoord)
      const tx = au | 0;
      const ty = av | 0;

      // compute the mix amounts between pixels
      const t1 = au % 1;
      const t2 = av % 1;

      // get the 4 pixels
      const tl = getSrcPixel(tx, ty);
      const tr = getSrcPixel(tx + 1, ty);
      const bl = getSrcPixel(tx, ty + 1);
      const br = getSrcPixel(tx + 1, ty + 1);

      // copy the "sampled" result into the dest.
      const dstOffset = (y * dstWidth + x) * 4;
      dst.set(bilinearFilter(tl, tr, bl, br, t1, t2), dstOffset);
    }
  }
  return { data: dst, width: dstWidth, height: dstHeight };
};

const generateMips = (src, srcWidth) => {
  const srcHeight = src.length / 4 / srcWidth;

  // populate with first mip level (base level)
  let mip = { data: src, width: srcWidth, height: srcHeight, };
  const mips = [mip];

  while (mip.width > 1 || mip.height > 1) {
    mip = createNextMipLevelRgba8Unorm(mip);
    mips.push(mip);
  }
  return mips;
};
```

We'll go over how to do this on the GPU in [another article](webgpu-importing-textures.html).
For now, we can use the code above to generate a mipmap.

We pass our texture data to the function above, and it returns an array of mip level data.
We can then create a texture with all the mip levels

```js
  const mips = generateMips(textureData, kTextureWidth);

  const texture = device.createTexture({
    label: 'yellow F on red',
+    size: [mips[0].width, mips[0].height],
+    mipLevelCount: mips.length,
    format: 'rgba8unorm',
    usage:
      GPUTextureUsage.TEXTURE_BINDING |
      GPUTextureUsage.COPY_DST,
  });
  mips.forEach(({data, width, height}, mipLevel) => {
    device.queue.writeTexture(
-      { texture },
-      textureData,
-      { bytesPerRow: kTextureWidth * 4 },
-      { width: kTextureWidth, height: kTextureHeight },
+      { texture, mipLevel },
+      data,
+      { bytesPerRow: width * 4 },
+      { width, height },
    );
  });
```

Notice we pass in `mipLevelCount` to the number of mip levels. WebGPU will then
create the correct sized mip level at each level. We then copy the data to each
level by specifying the `mipLevel`

Let's also add a scale setting so we can see the quad drawn at different sizes.

```js
  const settings = {
    addressModeU: 'repeat',
    addressModeV: 'repeat',
    magFilter: 'linear',
    minFilter: 'linear',
+    scale: 1,
  };

  ...

  const gui = new GUI();
  Object.assign(gui.domElement.style, {right: '', left: '15px'});
  gui.add(settings, 'addressModeU', addressOptions);
  gui.add(settings, 'addressModeV', addressOptions);
  gui.add(settings, 'magFilter', filterOptions);
  gui.add(settings, 'minFilter', filterOptions);
+  gui.add(settings, 'scale', 0.5, 6);

  function render(time) {

    ...

-    const scaleX = 4 / canvas.width;
-    const scaleY = 4 / canvas.height;
+    const scaleX = 4 / canvas.width * settings.scale;
+    const scaleY = 4 / canvas.height * settings.scale;

```

And with that the GPU is choosing the smallest mip to draw and the flickering is
gone.

{{{example url="../webgpu-simple-textured-quad-mipmap.html"}}}

Adjust the scale and you can see as we get bigger, which mip level is used
changes. There's a pretty harsh transition between scale 2.4 and scale 2.5
where the GPU switches between mip level 0 (the largest mip level) and
mip level 1 (the middle size). What to do about that?

## <a id="a-mipmap-filter"></a>mipmapFilter

Just like we have a `magFilter` and a `minFilter` both of which can be `nearest`
or `linear`, there is also a `mipmapFilter` setting which can also be `nearest`
or `linear`.

This chooses if we blend between mip levels. In `mipmapFilter: 'linear'`, colors
are sampled from 2 mip levels, either with nearest or linear filtering based on
the previous settings, then, those 2 colors are again `mix`ed in a similar way.

This comes up most when drawing things in 3D. How to draw in 3D is covered in
[other articles](webgpu-perspective.html) so I'm not going to cover that here
but we'll change our previous example to show some 3D so we can see better
how `mipmapFilter` works.

First let's make some textures. We'll make one 16x16 texture which I think will
better show `mipmapFilter`'s effect.

```js
  const createBlendedMipmap = () => {
    const w = [255, 255, 255, 255];
    const r = [255,   0,   0, 255];
    const b = [  0,  28, 116, 255];
    const y = [255, 231,   0, 255];
    const g = [ 58, 181,  75, 255];
    const a = [ 38, 123, 167, 255];
    const data = new Uint8Array([
      w, r, r, r, r, r, r, a, a, r, r, r, r, r, r, w,
      w, w, r, r, r, r, r, a, a, r, r, r, r, r, w, w,
      w, w, w, r, r, r, r, a, a, r, r, r, r, w, w, w,
      w, w, w, w, r, r, r, a, a, r, r, r, w, w, w, w,
      w, w, w, w, w, r, r, a, a, r, r, w, w, w, w, w,
      w, w, w, w, w, w, r, a, a, r, w, w, w, w, w, w,
      w, w, w, w, w, w, w, a, a, w, w, w, w, w, w, w,
      b, b, b, b, b, b, b, b, a, y, y, y, y, y, y, y,
      b, b, b, b, b, b, b, g, y, y, y, y, y, y, y, y,
      w, w, w, w, w, w, w, g, g, w, w, w, w, w, w, w,
      w, w, w, w, w, w, r, g, g, r, w, w, w, w, w, w,
      w, w, w, w, w, r, r, g, g, r, r, w, w, w, w, w,
      w, w, w, w, r, r, r, g, g, r, r, r, w, w, w, w,
      w, w, w, r, r, r, r, g, g, r, r, r, r, w, w, w,
      w, w, r, r, r, r, r, g, g, r, r, r, r, r, w, w,
      w, r, r, r, r, r, r, g, g, r, r, r, r, r, r, w,
    ].flat());
    return generateMips(data, 16);
  };
```

This will generate these mip levels

<div class="webgpu-center center diagram"><div data-diagram="blended-mips" style="display: inline-block;"></div></div>

We're free to put any data in each mip level so another good way to see what's happening
is to make each mip level different colors. Let's use the canvas 2d api to make mip levels.

```js
  const createCheckedMipmap = () => {
    const ctx = document.createElement('canvas').getContext('2d', {willReadFrequently: true});
    const levels = [
      { size: 64, color: 'rgb(128,0,255)', },
      { size: 32, color: 'rgb(0,255,0)', },
      { size: 16, color: 'rgb(255,0,0)', },
      { size:  8, color: 'rgb(255,255,0)', },
      { size:  4, color: 'rgb(0,0,255)', },
      { size:  2, color: 'rgb(0,255,255)', },
      { size:  1, color: 'rgb(255,0,255)', },
    ];
    return levels.map(({size, color}, i) => {
      ctx.canvas.width = size;
      ctx.canvas.height = size;
      ctx.fillStyle = i & 1 ? '#000' : '#fff';
      ctx.fillRect(0, 0, size, size);
      ctx.fillStyle = color;
      ctx.fillRect(0, 0, size / 2, size / 2);
      ctx.fillRect(size / 2, size / 2, size / 2, size / 2);
      return ctx.getImageData(0, 0, size, size);
    });
  };
```

This code will generate these mip levels.

<div class="webgpu-center center diagram"><div data-diagram="checkered-mips" style="display: inline-block;"></div></div>

Now that we've created the data lets create the textures

```js
+  const createTextureWithMips = (mips, label) => {
    const texture = device.createTexture({
-      label: 'yellow F on red',
+      label,
      size: [mips[0].width, mips[0].height],
      mipLevelCount: mips.length,
      format: 'rgba8unorm',
      usage:
        GPUTextureUsage.TEXTURE_BINDING |
        GPUTextureUsage.COPY_DST,
    });
    mips.forEach(({data, width, height}, mipLevel) => {
      device.queue.writeTexture(
          { texture, mipLevel },
          data,
          { bytesPerRow: width * 4 },
          { width, height },
      );
    });
    return texture;
+  };

+  const textures = [
+    createTextureWithMips(createBlendedMipmap(), 'blended'),
+    createTextureWithMips(createCheckedMipmap(), 'checker'),
+  ];
```

We're going to draw a quad extending into the distance in 8 location. 
We'll use matrix math as covered in [the series of articles on 3D](webgpu-cameras.html).

```wsgl
struct OurVertexShaderOutput {
  @builtin(position) position: vec4f,
  @location(0) texcoord: vec2f,
};

struct Uniforms {
-  scale: vec2f,
-  offset: vec2f,
+  matrix: mat4x4f,
};

@group(0) @binding(2) var<uniform> uni: Uniforms;

@vertex fn vs(
  @builtin(vertex_index) vertexIndex : u32
) -> OurVertexShaderOutput {
  let pos = array(

    vec2f( 0.0,  0.0),  // center
    vec2f( 1.0,  0.0),  // right, center
    vec2f( 0.0,  1.0),  // center, top

    // 2st triangle
    vec2f( 0.0,  1.0),  // center, top
    vec2f( 1.0,  0.0),  // right, center
    vec2f( 1.0,  1.0),  // right, top
  );

  var vsOutput: OurVertexShaderOutput;
  let xy = pos[vertexIndex];
-  vsOutput.position = vec4f(xy * uni.scale + uni.offset, 0.0, 1.0);
+  vsOutput.position = uni.matrix * vec4f(xy, 0.0, 1.0);
  vsOutput.texcoord = xy * vec2f(1, 50);
  return vsOutput;
}

@group(0) @binding(0) var ourSampler: sampler;
@group(0) @binding(1) var ourTexture: texture_2d<f32>;

@fragment fn fs(fsInput: OurVertexShaderOutput) -> @location(0) vec4f {
  return textureSample(ourTexture, ourSampler, fsInput.texcoord);
}
```

Each of the 8 planes will use different combinations of `minFilter`, `magFilter`
and `mipmapFilter`. That means each one needs a different bind group that
contains a sampler with that specific combination of filters. Further, we have 2
textures. Textures are part of the bind group as well so we'll need 2 bind
groups per object, one for each texture. We can then select which one to use
when we render. To draw the plane in 8 locations we'll also need one uniform
buffer per location like we covered in [the article on uniforms](webgpu-uniforms.html). 

```js
  // offsets to the various uniform values in float32 indices
  const kMatrixOffset = 0;

  const objectInfos = [];
  for (let i = 0; i < 8; ++i) {
    const sampler = device.createSampler({
      addressModeU: 'repeat',
      addressModeV: 'repeat',
      magFilter: (i & 1) ? 'linear' : 'nearest',
      minFilter: (i & 2) ? 'linear' : 'nearest',
      mipmapFilter: (i & 4) ? 'linear' : 'nearest',
    });

    // create a buffer for the uniform values
    const uniformBufferSize =
      16 * 4; // matrix is 16 32bit floats (4bytes each)
    const uniformBuffer = device.createBuffer({
      label: 'uniforms for quad',
      size: uniformBufferSize,
      usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,
    });

    // create a typedarray to hold the values for the uniforms in JavaScript
    const uniformValues = new Float32Array(uniformBufferSize / 4);
    const matrix = uniformValues.subarray(kMatrixOffset, 16);

    const bindGroups = textures.map(texture =>
      device.createBindGroup({
        layout: pipeline.getBindGroupLayout(0),
        entries: [
          { binding: 0, resource: sampler },
          { binding: 1, resource: texture.createView() },
          { binding: 2, resource: { buffer: uniformBuffer }},
        ],
      }));

    // Save the data we need to render this object.
    objectInfos.push({
      bindGroups,
      matrix,
      uniformValues,
      uniformBuffer,
    });
  }
```

At render time we [compute a viewProjection matrix](webgpu-cameras.html).

```js
  function render() {
    const fov = 60 * Math.PI / 180;  // 60 degrees in radians
    const aspect = canvas.clientWidth / canvas.clientHeight;
    const zNear  = 1;
    const zFar   = 2000;
    const projectionMatrix = mat4.perspective(fov, aspect, zNear, zFar);

    const cameraPosition = [0, 0, 2];
    const up = [0, 1, 0];
    const target = [0, 0, 0];
    const cameraMatrix = mat4.lookAt(cameraPosition, target, up);
    const viewMatrix = mat4.inverse(cameraMatrix);
    const viewProjectionMatrix = mat4.multiply(projectionMatrix, viewMatrix);

    ...
```

Then for each plane, we select a bind group based on which texture we want to show
and compute a unique matrix to position that plane.

```js
  let texNdx = 0;

  function render() {
    ...

    const pass = encoder.beginRenderPass(renderPassDescriptor);
    pass.setPipeline(pipeline);

    objectInfos.forEach(({bindGroups, matrix, uniformBuffer, uniformValues}, i) => {
      const bindGroup = bindGroups[texNdx];

      const xSpacing = 1.2;
      const ySpacing = 0.7;
      const zDepth = 50;

      const x = i % 4 - 1.5;
      const y = i < 4 ? 1 : -1;

      mat4.translate(viewProjectionMatrix, [x * xSpacing, y * ySpacing, -zDepth * 0.5], matrix);
      mat4.rotateX(matrix, 0.5 * Math.PI, matrix);
      mat4.scale(matrix, [1, zDepth * 2, 1], matrix);
      mat4.translate(matrix, [-0.5, -0.5, 0], matrix);

      // copy the values from JavaScript to the GPU
      device.queue.writeBuffer(uniformBuffer, 0, uniformValues);

      pass.setBindGroup(0, bindGroup);
      pass.draw(6);  // call our vertex shader 6 times
    });

    pass.end();
```

I removed the existing UI code, switched back from a rAF loop to rendering
in the `ResizeObserver` callback, and stopped making the resolution low.

```js
-  function render(time) {
-    time *= 0.001;
+  function render() {

    ...

-    requestAnimationFrame(render);
  }
-  requestAnimationFrame(render);

  const observer = new ResizeObserver(entries => {
    for (const entry of entries) {
      const canvas = entry.target;
-      const width = entry.contentBoxSize[0].inlineSize / 64 | 0;
-      const height = entry.contentBoxSize[0].blockSize / 64 | 0;
+      const width = entry.contentBoxSize[0].inlineSize;
+      const height = entry.contentBoxSize[0].blockSize;
      canvas.width = Math.max(1, Math.min(width, device.limits.maxTextureDimension2D));
      canvas.height = Math.max(1, Math.min(height, device.limits.maxTextureDimension2D));
+      render();
    }
  });
  observer.observe(canvas);
```

Since we're no longer low-res we can get rid of the CSS that was preventing the browser
from filtering the canvas itself.

```css
canvas {
  display: block;  /* make the canvas act like a block   */
  width: 100%;     /* make the canvas fill its container */
  height: 100%;
-  image-rendering: pixelated;
-  image-rendering: crisp-edges;
}
```

And we can make it so if you click the canvas it switches which texture to
draw with and re-renders

```js
  canvas.addEventListener('click', () => {
    texNdx = (texNdx + 1) % textures.length;
    render();
  });
```

{{{example url="../webgpu-simple-textured-quad-mipmapfilter.html"}}}

Hopefully you can see the progression from the top left with all filtering
set to `nearest` to the bottom right where all filtering is set to `linear`.
In particular, since we added `mipmapFilter` in this example, if you click
the image to show the checked texture where every mip level is a different
color, you should be able to see that every plane at the top has
`mipmapFilter` set to `nearest` so the point when switching from one mip level
to the next is abrupt. On the bottom, each plane has `mipmapFilter` set to
`linear` so blending happens between the mip levels.

You might wonder, why not always set all filtering to `linear`? The obvious
reason is style. If you're trying to make a pixelated looking image then
of course you might not want filtering. Another is speed. Reading 1 pixel
from a texture when all filtering is set to nearest is faster then reading
8 pixels from a texture when all filtering is set to linear.

TBD: Repeat

TBD: Anisotropic filtering

## Texture Types and Texture Views

Until this point we've only used 2d textures. There are 3 types of textures

* "1d"
* "2d"
* "3d"

In some way you can *kind of* consider a "2d" texture just a "3d" texture with a
depth of 1. And a "1d" texture is just a "2d" texture with a height of 1. Two
actual differences, textures are limited in their maximum allowed dimensions. The
limit is different for each type of texture "1d", "2d", and "3d". We've used the
"2d" limit when setting the size of the canvas.

```js
canvas.width = Math.max(1, Math.min(width, device.limits.maxTextureDimension2D));
canvas.height = Math.max(1, Math.min(height, device.limits.maxTextureDimension2D));
```

Another is speed, at least for a 3d texture vs a 2d texture, with all the
sampler filters set to `linear`, sampling a 3d texture would require looking at
16 texels and blending them all together. Sampling a 2d texture only needs 8
texels. It's possible a 1d texture only needs 4 but I have no idea if any GPUs
actually optimize for 1d textures.

### Texture Views

There are 6 types of texture views

* "1d"
* "2d"
* "2d-array"
* "3d"
* "cube"
* "cube-array"

"1d" textures can only have a "1d" view. "3d" textures can only have a "3d" view.
"2d" texture can have a "2d-array" view. If a "2d" texture has 6 layers it can
have a "cube" view. If it has a multiple of 6 layers it can have a "cube-array"
view. You can choose how to view a texture when you call `someTexture.createView`.
Texture views default to the same as their dimension but you can pass a different
dimension to `someTexture.createView`.

We'll cover "3d" textures [in the article on tone mapping / 3dLUTs](webgpu-3dluts.html)

A "cube" texture is a texture that represents the 6 faces of a cube. Cube textures
are often used to draw sky boxes and for reflections and environment maps. We'll cover
that in [the article on cube maps](webgpu-cube-maps.html)

A "2d-array" is an array of 2d textures. You can then choose which texture of
the array to access in your shader. They are commonly used for terrain rendering
among other things.

A "cube-array" is an array of cube textures.

Each type of texture has its own corresponding type in WGSL.

<div class="webgpu_center data-table" style="max-width: 500px;">
  <style>
    .texture-type {
      text-align: left;
      font-size: large;
      line-height: 1.5em;
    }
    .texture-type td:nth-child(1) {
      white-space: nowrap;
    }
  </style>
  <table class="texture-type">
   <thead>
    <tr>
     <th>type</th>
     <th>WGSL types</th>
    </tr>
   </thead>
   <tbody>
    <tr><td>"1d"</td><td><code>texture_1d</code> or <code>texture_storage_1d</code></td></tr>
    <tr><td>"2d"</td><td><code>texture_2d</code> or <code>texture_storage_2d</code> or <code>texture_multisampled_2d</code> as well as a special case for in certain situations <code>texture_depth_2d</code> and <code>texture_depth_multisampled_2d</code></td></tr>
    <tr><td>"2d-array"</td><td><code>texture_2d_array</code> or <code>texture_storage_2d_array</code> and sometimes <code>texture_depth_2d_array</code></td></tr>
    <tr><td>"3d"</td><td><code>texture_3d</code> or <code>texture_storage_3d</code></td></tr>
    <tr><td>"cube"</td><td><code>texture_cube</code> and sometimes <code>texture_depth_cube</code></td></tr>
    <tr><td>"cube-array"</td><td><code>texture_cube_array</code> and sometimes <code>texture_depth_cube_array</code></td></tr>
   </tbody>
  </table>
</div>

We'll cover some of this in actual use but, it can be a little confusing that
when creating a texture (calling `device.createTexture`), there is only "1d",
"2d", or "3d" as options and the default is "2d" so we have not had to specify
the dimensions yet.

## Texture Formats

For now, this is the basics of textures.
Textures are a huge topic and there's a bunch more to cover.

We've used `rgba8unorm` textures through out this article but there are
a ton of different texture formats.

Here are the "color" formats though of course you don't have to store colors in them.

<div class="webgpu_center data-table"><div data-diagram="color-texture-formats"></div></div>

To read a format, like "rg16float". the first letters are the channels supported
in the texture so "rg16float" supports "rg" or red and green (2 channels). The
number, 16, means those channels are 16bits each. The word at the end is what
kind of data is in the channel. "float" is floating point data.

"unorm" is unsigned normalized data (0 to 1) meaning the data in the texture
goes from 0 to N where N is the maximum integer value for that number of bits.
That range of integers is then interpreted as a floating point range of (0 to
1). In other words, for an 8unorm texture, that's 8 bits (so values from 0 to
255) that get interpreted as values from (0 to 1).

"snorm" is signed normalized data (-1 to +1) so the range of data goes from the
most negative integer represented by the number of bits to the most positive.For
example 8snorm is 8bits. As a signed integer the lowest number would be -128 and
the highest is +127. That range gets converted to (-1 to +1).

"sint" is signed integers. "uint" is unsigned integer. If there are multiple
letter number combinations it's specifying the number of bits for each channel.
For example "rg11b10ufloat" is "rg11" so 11bits each of red and green. "b10" so
10bits of blue and they are all unsigned floating point numbers.

* **renderable**

  True means you can render to it (set its usage to `GPUTextureUsage.RENDER_ATTACHMENT`)

* **multisample**

  Can be [multisampled](webgpu-multisampling.html)

* **storage**

  Can be written to as a [storage texture](webgpu-storage-textures.html)

* **sampler type**

  This has implications for what type of texture you need to declare it in WGSL
  and how you bind a sampler to a bind group. Above we used `texture_2d<f32>`
  but for example, `sint` would need `texture_2d<i32>` and `uint` would need
  `texture_2d<u32>` in WGSL.

  In the sampler type column, `unfilterable-float` means your sampler can only
  use `nearest` for that format and it means you may have to manually
  create a bind group layout, something we haven't done before as we've been
  using `'auto'` layout. This mostly exists because desktop GPU can generally
  filter 32bit floating point textures but, at least as of 2023, most mobile
  devices can not. If your adaptor supports the `float32-filterable`
  [feature](webgpu-limits-and-features.html) and you enable it when requesting a
  device then the formats `r32float`, `rg32float`, and `rgba32float` switch from
  `unfilterable-float` to `float` and these textures formats will work with no
  other changes.

<a id="a-depth-stencil-formats"></a>And here are the depth and stencil formats

<div class="webgpu_center data-table"><div data-diagram="depth-stencil-texture-formats"></div></div>

* **feature**

  means this [*optional* feature](webgpu-limits-and-features.html) is required to use this format.

* **copy src**

  Whether you're allowed to specify `GPUTextureUsage.COPY_SRC`

* **copy dst**

  Whether you're allowed to specify `GPUTextureUsage.COPY_DST`

We'll use a depth texture in [an article in the series on 3d](webgpu-orthographic-projection.html) as well
as [the article about shadow maps](webgpu-shadow-maps.html).

There's also a bunch compressed texture formats which we'll save for another article.

Let's cover [importing external textures](webgpu-importing-textures.html) next.

<!-- keep this at the bottom of the article -->
<script type="module" src="/3rdparty/pixel-perfect.js"></script>
<script type="module" src="webgpu-textures.js"></script>
