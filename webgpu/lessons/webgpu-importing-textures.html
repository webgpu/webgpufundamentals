<!DOCTYPE html><!-- this file is auto-generated from webgpu/lessons/webgpu-importing-textures.md. Do not edited directly --><!--
Copyright 2023, GfxFundamentals Contributors
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

* Redistributions of source code must retain the above copyright
  notice, this list of conditions and the following disclaimer.

* Redistributions in binary form must reproduce the above
  copyright notice, this list of conditions and the following disclaimer
  in the documentation and/or other materials provided with the
  distribution.

* Neither the name of GfxFundamentals nor the names of the
  contributors may be used to endorse or promote products derived from
  this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


--><html lang="en"><head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="description" content="How to load an Image/Canvas/Video into a texture">
<meta name="keywords" content="webgpu graphics">
<meta name="thumbnail" content="https://webgpufundamentals.org/webgpu/lessons/screenshots/webgpu-importing-textures_en.jpg">

<meta property="og:title" content="WebGPU Loading Images into Textures">
<meta property="og:type" content="website">
<meta property="og:image" content="https://webgpufundamentals.org/webgpu/lessons/screenshots/webgpu-importing-textures_en.jpg">
<meta property="og:description" content="How to load an Image/Canvas/Video into a texture">
<meta property="og:url" content="https://webgpufundamentals.org/webgpu/lessons/webgpu-importing-textures.html">

<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@greggman">
<meta name="twitter:creator" content="@greggman">
<meta name="twitter:domain" content="webgpufundamentals.org">
<meta name="twitter:title" content="WebGPU Loading Images into Textures">
<meta name="twitter:url" content="https://webgpufundamentals.org/webgpu/lessons/webgpu-importing-textures.html">
<meta name="twitter:description" content="How to load an Image/Canvas/Video into a texture">
<meta name="twitter:image:src" content="https://webgpufundamentals.org/webgpu/lessons/screenshots/webgpu-importing-textures_en.jpg">

<script type="application/ld+json">
{
  "@context":"https://schema.org",
  "@graph":[
    {
      "@type":"WebSite",
      "@id":"https://webgpufundamentals.org/#website",
      "url":"https://webgpufundamentals.org/",
      "name":"webgpufundamentals"
    },
    {
      "@type":"ImageObject",
      "@id":"https://webgpufundamentals.org/webgpu/lessons/webgpu-importing-textures.html#primaryimage",
      "url":"https://webgpufundamentals.org/webgpu/lessons/screenshots/webgpu-importing-textures_en.jpg",
      "width":1200,
      "height":630
    },
    {
      "@type":"WebPage",
      "@id":"https://webgpufundamentals.org/webgpu/lessons/webgpu-importing-textures.html#webpage",
      "url":"https://webgpufundamentals.org/webgpu/lessons/webgpu-importing-textures.html",
      "inLanguage":"en",
      "name":"WebGPU Loading Images into Textures",
      "keywords":"webgpu graphics programming",
      "isPartOf":{
        "@id":"https://webgpufundamentals.org/#website"
      },
      "primaryImageOfPage":{
        "@id":"https://webgpufundamentals.org/webgpu/lessons/webgpu-importing-textures.html#primaryimage"
      }
    }
  ]
}
</script>

<title>WebGPU Loading Images into Textures</title>
<link href="/webgpu/lessons/resources/webgpufundamentals-icon.png" rel="shortcut icon" type="image/png">
<link rel="apple-touch-icon" href="/webgpu/lessons/resources/webgpufundamentals-icon.png">
<link rel="icon" href="/webgpu/lessons/resources/webgpufundamentals-icon.png">

<link rel="stylesheet" href="/webgpu/lessons/lang.css">
<link rel="stylesheet" href="/webgpu/lessons/resources/lesson.css">
</head>
<body>
<div class="webgpu_navbar">
  <div>
    <select class="language">
    <option value="/webgpu/lessons/webgpu-importing-textures.html" selected="">English
    </option><option value="/webgpu/lessons/es/webgpu-importing-textures.html">Spanish
    </option><option value="/webgpu/lessons/ja/webgpu-importing-textures.html">Êó•Êú¨Ë™û
    </option><option value="/webgpu/lessons/ko/webgpu-importing-textures.html">ÌïúÍµ≠Ïñ¥
    </option><option value="/webgpu/lessons/ru/webgpu-importing-textures.html">–†—É—Å—Å–∫–∏–π
    </option><option value="/webgpu/lessons/uk/webgpu-importing-textures.html">–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞
    </option><option value="/webgpu/lessons/zh_cn/webgpu-importing-textures.html">ÁÆÄ‰Ωì‰∏≠Êñá
</option></select>


    <a href="#toc">Table of Contents</a>
  </div>
</div>
<div class="webgpu_header">
  <h1><a href="/">webgpufundamentals.org</a></h1>
<style>
#forkongithub>div {
    background: #000;
    color: #fff;
    font-family: arial,sans-serif;
    text-align: center;
    font-weight: bold;
    padding: 5px 40px;
    font-size: 0.9rem;
    line-height: 1.3rem;
    position: relative;
    transition: 0.5s;
    display: block;
    width: 400px;
    position: absolute;
    top: 0;
    right: 0;
    transform: translateX(160px) rotate(45deg) translate(10px,70px);
    box-shadow: 4px 4px 10px rgba(0,0,0,0.8);
    pointer-events: auto;
}
#forkongithub a {
  text-decoration: none;
  color: #fff;
}
#forkongithub>div:hover {
    background: #c11;
    color: #fff;
}
#forkongithub .contributors {
  font-size: 0.75rem;
  background: rgba(255,255,255,0.2);
  line-height: 1.2;
  padding: 0.1em;
}
#forkongithub>div::before,#forkongithub>div::after {
    content: "";
    width: 100%;
    display: block;
    position: absolute;
    top: 1px;
    left: 0;
    height: 1px;
    background: #fff;
}
#forkongithub>div::after {
    bottom: 1px;
    top: auto;
}

#forkongithub{
    z-index: 9999;
    /* needed for firefox */
    overflow: hidden;
    width: 300px;
    height: 300px;
    position: absolute;
    right: 0;
    top: 0;
    pointer-events: none;
}
#forkongithub svg{
  width: 1em;
  height: 1em;
  vertical-align: middle;
}
#forkongithub img {
  width: 1em;
  height: 1em;
  border-radius: 100%;
  vertical-align: middle;
}

@media (max-width: 900px) {
    #forkongithub>div {
        line-height: 1.2rem;
    }
}
@media (max-width: 700px) {
  #forkongithub {
    display: none;
  }
}
@media (max-width: 410px) {
    #forkongithub>div {
        font-size: 0.7rem;
        transform: translateX(150px) rotate(45deg) translate(20px,40px);
    }
}

</style>
<div id="forkongithub">
  <div>
    <div><a href="https://github.com/webgpu/webgpufundamentals">Fix, Fork, Contribute <!--?xml version="1.0" encoding="UTF-8" standalone="no"?-->

<svg width="100%" height="100%" viewBox="0 0 136 133" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" xmlns:serif="http://www.serif.com/" style="fill-rule:evenodd;clip-rule:evenodd;stroke-linejoin:round;stroke-miterlimit:2;">
    <g transform="matrix(3.92891,0,0,3.92891,67.867,129.125)">
        <path d="M0,-31.904C-8.995,-31.904 -16.288,-24.611 -16.288,-15.614C-16.288,-8.417 -11.621,-2.312 -5.148,-0.157C-4.333,-0.008 -4.036,-0.511 -4.036,-0.943C-4.036,-1.329 -4.05,-2.354 -4.058,-3.713C-8.589,-2.729 -9.545,-5.897 -9.545,-5.897C-10.286,-7.779 -11.354,-8.28 -11.354,-8.28C-12.833,-9.29 -11.242,-9.27 -11.242,-9.27C-9.607,-9.155 -8.747,-7.591 -8.747,-7.591C-7.294,-5.102 -4.934,-5.821 -4.006,-6.238C-3.858,-7.29 -3.438,-8.008 -2.972,-8.415C-6.589,-8.826 -10.392,-10.224 -10.392,-16.466C-10.392,-18.244 -9.757,-19.698 -8.715,-20.837C-8.883,-21.249 -9.442,-22.905 -8.556,-25.148C-8.556,-25.148 -7.188,-25.586 -4.076,-23.478C-2.777,-23.84 -1.383,-24.02 0.002,-24.026C1.385,-24.02 2.779,-23.84 4.08,-23.478C7.19,-25.586 8.555,-25.148 8.555,-25.148C9.444,-22.905 8.885,-21.249 8.717,-20.837C9.761,-19.698 10.392,-18.244 10.392,-16.466C10.392,-10.208 6.583,-8.831 2.954,-8.428C3.539,-7.925 4.06,-6.931 4.06,-5.411C4.06,-3.234 4.04,-1.477 4.04,-0.943C4.04,-0.507 4.333,0 5.16,-0.159C11.628,-2.318 16.291,-8.419 16.291,-15.614C16.291,-24.611 8.997,-31.904 0,-31.904" style="fill:white;"></path>
    </g>
</svg>
</a></div>
  </div>
</div>

</div>


<div class="container">
  <div class="lesson-title">
    <h1>WebGPU Loading Images into Textures</h1>
  </div>
  <div class="lesson">
    <div class="lesson-main">
      <p>We covered some basics about using textures <a href="webgpu-textures.html">in the previous article</a>.
In this article we‚Äôll cover loading an image into a texture
as well as generating mipmaps on the GPU.</p>
<p>In the previous article we‚Äôd created a texture by calling <code class="notranslate" translate="no">device.createTexture</code> and then
put data in the texture by calling <code class="notranslate" translate="no">device.queue.writeTexture</code>. There‚Äôs another function
on <code class="notranslate" translate="no">device.queue</code> called <code class="notranslate" translate="no">device.queue.copyExternalImageToTexture</code> that let‚Äôs us copy
an image into a texture.</p>
<p>It can take an <a href="https://developer.mozilla.org/en-US/docs/Web/API/ImageBitmap"><code class="notranslate" translate="no">ImageBitmap</code></a> so let‚Äôs take <a href="webgpu-textures.html#a-mag-filter">the magFilter example from the previous article</a> and change it to load a few images.</p>
<p>First we need some code to get an <a href="https://developer.mozilla.org/en-US/docs/Web/API/ImageBitmap"><code class="notranslate" translate="no">ImageBitmap</code></a> from an image</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">  async function loadImageBitmap(url) {
    const res = await fetch(url);
    const blob = await res.blob();
    return await createImageBitmap(blob, { colorSpaceConversion: 'none' });
  }
</pre>
<p>The code above calls <a href="https://developer.mozilla.org/en-US/docs/Web/API/fetch"><code class="notranslate" translate="no">fetch</code></a> with the url of an image. This returns a <a href="https://developer.mozilla.org/en-US/docs/Web/API/Response"><code class="notranslate" translate="no">Response</code></a>. We then
use that to load a <a href="https://developer.mozilla.org/en-US/docs/Web/API/Blob"><code class="notranslate" translate="no">Blob</code></a> which opaquely represents the data of the image file. We then pass
that to <a href="https://developer.mozilla.org/en-US/docs/Web/API/createImageBitmap"><code class="notranslate" translate="no">createImageBitmap</code></a> which is a standard browser function to create an <a href="https://developer.mozilla.org/en-US/docs/Web/API/ImageBitmap"><code class="notranslate" translate="no">ImageBitmap</code></a>.
We pass <code class="notranslate" translate="no">{ colorSpaceConversion: 'none' }</code> to tell the browser not to apply any color space. It‚Äôs up to you if
you want the browser to apply a color space or not. Often in WebGPU we might load
an image that is a normal map or a height map or something that is not color data.
In those cases we definitely don‚Äôt want the browser to muck with the data in the image.</p>
<p>Now that we have code to create an <a href="https://developer.mozilla.org/en-US/docs/Web/API/ImageBitmap"><code class="notranslate" translate="no">ImageBitmap</code></a> let‚Äôs load one and create a texture of the same size.</p>
<p>We‚Äôll load this image</p>
<div class="webgpu_center"><img src="../resources/images/f-texture.png"></div>
<p>I was taught once that a texture with an <code class="notranslate" translate="no">F</code> in it is a good example texture because we can instantly
see its orientation.</p>
<div class="webgpu_center"><img src="resources/f-orientation.svg"></div>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">-  const texture = device.createTexture({
-    label: 'yellow F on red',
-    size: [kTextureWidth, kTextureHeight],
-    format: 'rgba8unorm',
-    usage:
-      GPUTextureUsage.TEXTURE_BINDING |
-      GPUTextureUsage.COPY_DST,
-  });
+  const url = 'resources/images/f-texture.png';
+  const source = await loadImageBitmap(url);
+  const texture = device.createTexture({
+    label: url,
+    format: 'rgba8unorm',
+    size: [source.width, source.height],
+    usage: GPUTextureUsage.TEXTURE_BINDING |
+           GPUTextureUsage.COPY_DST |
+           GPUTextureUsage.RENDER_ATTACHMENT,
+  });
</pre>
<p>Note that <code class="notranslate" translate="no">copyExternalImageToTexture</code> requires that we include to
<code class="notranslate" translate="no">GPUTextureUsage.COPY_DST</code> and <code class="notranslate" translate="no">GPUTextureUsage.RENDER_ATTACHMENT</code>
usage flags.</p>
<p>So then we can copy the <a href="https://developer.mozilla.org/en-US/docs/Web/API/ImageBitmap"><code class="notranslate" translate="no">ImageBitmap</code></a> to the texture</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">-  device.queue.writeTexture(
-      { texture },
-      textureData,
-      { bytesPerRow: kTextureWidth * 4 },
-      { width: kTextureWidth, height: kTextureHeight },
-  );
+  device.queue.copyExternalImageToTexture(
+    { source, flipY: true },
+    { texture },
+    { width: source.width, height: source.height },
+  );
</pre>
<p>The parameters to <code class="notranslate" translate="no">copyExternalImageToTexture</code> are
The source, the destination, the size. For the source
we can specify <code class="notranslate" translate="no">flipY: true</code> if we want the texture flipped on load.</p>
<p>And that works!</p>
<p></p><div class="webgpu_example_container">
  <div><iframe class="webgpu_example" style=" " src="/webgpu/resources/editor.html?url=/webgpu/lessons/..%2Fwebgpu-simple-textured-quad-import-no-mips.html"></iframe></div>
  <a class="webgpu_center" href="/webgpu/lessons/../webgpu-simple-textured-quad-import-no-mips.html" target="_blank">click here to open in a separate window</a>
</div>

<p></p>
<h2 id="generating-mips-on-the-gpu"><a id="a-generating-mips-on-the-gpu"></a>Generating mips on the GPU</h2>
<p>In <a href="webgpu-textures.html#a-mipmap-filter">the previous article we also generated a mipmap</a>
but in that case we had easy access to the image data. When loading an image, we
could draw that image into a 2D canvas, the call <code class="notranslate" translate="no">getImageData</code> to get the data, and
finally generate mips and upload. That would be pretty slow. It would also potentially
be lossy since how canvas 2D renders is intentionally implementation dependant.</p>
<p>When we generated mip levels we did a bilinear interpolation which is exactly what
the GPU does with <code class="notranslate" translate="no">minFilter: linear</code>. We can use that feature to generate mip levels
on the GPU</p>
<p>Let‚Äôs modify the <a href="webgpu-textures.html#a-mipmap-filter">mipmapFilter example from the previous article</a>
to load images and generate mips using the GPU</p>
<p>First, let‚Äôs change the code that creates the texture to create mip levels. We need to know how many
to create which we can calculate like this</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">  const numMipLevels = (...sizes) =&gt; {
    const maxSize = Math.max(...sizes);
    return 1 + Math.log2(maxSize) | 0;
  };
</pre>
<p>We can call that with 1 or more numbers and it will return the number of mips needed, so for example
<code class="notranslate" translate="no">numMipLevels(123, 456)</code> returns <code class="notranslate" translate="no">9</code>.</p>
<blockquote>
<ul>
<li>level 0: 123, 456</li>
<li>level 1: 61, 228</li>
<li>level 2: 30, 114</li>
<li>level 3: 15, 57</li>
<li>level 4: 7, 28</li>
<li>level 5: 3, 14</li>
<li>level 6: 1, 7</li>
<li>level 7: 1, 3</li>
<li>level 8: 1, 1</li>
</ul>
<p>9 mip levels</p>
</blockquote>
<p><code class="notranslate" translate="no">Math.log2</code> tells us the power of 2 we need to make our number.
In other words, <code class="notranslate" translate="no">Math.log2(8) = 3</code> because 2<sup>3</sup> = 8. Another way to say the same thing is, <code class="notranslate" translate="no">Math.log2</code> tells us how
many times can we divide this number by 2.</p>
<blockquote>
<pre class="prettyprint showlinemods notranslate notranslate" translate="no">Math.log2(8)
          8 / 2 = 4
                  4 / 2 = 2
                          2 / 2 = 1
</pre>
</blockquote>
<p>So we can divide 8 by 2 three times. That‚Äôs exactly what we need to compute how many mip levels to make.
It‚Äôs <code class="notranslate" translate="no">Math.log2(largestSize) + 1</code>. 1 for the original size mip level 0</p>
<p>So, we can now create the right number of mip levels</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">  const texture = device.createTexture({
    label: url,
    format: 'rgba8unorm',
    mipLevelCount: numMipLevels(source.width, source.height),
    size: [source.width, source.height],
    usage: GPUTextureUsage.TEXTURE_BINDING |
           GPUTextureUsage.COPY_DST |
           GPUTextureUsage.RENDER_ATTACHMENT,
  });
  device.queue.copyExternalImageToTexture(
    { source, flipY: true, },
    { texture },
    { width: source.width, height: source.height },
  );
</pre>
<p>To generate the next mip level, we‚Äôll draw a textured quad, just like we‚Äôve been doing, from the
existing mip level, to the next level, with <code class="notranslate" translate="no">minFilter: linear</code>.</p>
<p>Here‚Äôs the code</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">  const generateMips = (() =&gt; {
    let sampler;
    let module;
    const pipelineByFormat = {};

    return function generateMips(device, texture) {
      if (!module) {
        module = device.createShaderModule({
          label: 'textured quad shaders for mip level generation',
          code: `
            struct VSOutput {
              @builtin(position) position: vec4f,
              @location(0) texcoord: vec2f,
            };

            @vertex fn vs(
              @builtin(vertex_index) vertexIndex : u32
            ) -&gt; VSOutput {
              let pos = array(

                vec2f( 0.0,  0.0),  // center
                vec2f( 1.0,  0.0),  // right, center
                vec2f( 0.0,  1.0),  // center, top

                // 2st triangle
                vec2f( 0.0,  1.0),  // center, top
                vec2f( 1.0,  0.0),  // right, center
                vec2f( 1.0,  1.0),  // right, top
              );

              var vsOutput: VSOutput;
              let xy = pos[vertexIndex];
              vsOutput.position = vec4f(xy * 2.0 - 1.0, 0.0, 1.0);
              vsOutput.texcoord = vec2f(xy.x, 1.0 - xy.y);
              return vsOutput;
            }

            @group(0) @binding(0) var ourSampler: sampler;
            @group(0) @binding(1) var ourTexture: texture_2d&lt;f32&gt;;

            @fragment fn fs(fsInput: VSOutput) -&gt; @location(0) vec4f {
              return textureSample(ourTexture, ourSampler, fsInput.texcoord);
            }
          `,
        });

        sampler = device.createSampler({
          minFilter: 'linear',
        });
      }

      if (!pipelineByFormat[texture.format]) {
        pipelineByFormat[texture.format] = device.createRenderPipeline({
          label: 'mip level generator pipeline',
          layout: 'auto',
          vertex: {
            module,
          },
          fragment: {
            module,
            targets: [{ format: texture.format }],
          },
        });
      }
      const pipeline = pipelineByFormat[texture.format];

      const encoder = device.createCommandEncoder({
        label: 'mip gen encoder',
      });

      let width = texture.width;
      let height = texture.height;
      let baseMipLevel = 0;
      while (width &gt; 1 || height &gt; 1) {
        width = Math.max(1, width / 2 | 0);
        height = Math.max(1, height / 2 | 0);

        const bindGroup = device.createBindGroup({
          layout: pipeline.getBindGroupLayout(0),
          entries: [
            { binding: 0, resource: sampler },
            { binding: 1, resource: texture.createView({baseMipLevel, mipLevelCount: 1}) },
          ],
        });

        ++baseMipLevel;

        const renderPassDescriptor = {
          label: 'our basic canvas renderPass',
          colorAttachments: [
            {
              view: texture.createView({baseMipLevel, mipLevelCount: 1}),
              loadOp: 'clear',
              storeOp: 'store',
            },
          ],
        };

        const pass = encoder.beginRenderPass(renderPassDescriptor);
        pass.setPipeline(pipeline);
        pass.setBindGroup(0, bindGroup);
        pass.draw(6);  // call our vertex shader 6 times
        pass.end();
      }

      const commandBuffer = encoder.finish();
      device.queue.submit([commandBuffer]);
    };
  })();
</pre>
<p>The code above looks long but it‚Äôs almost the exact same code we‚Äôve been using in our examples with textures so far.
What‚Äôs changed</p>
<ul>
<li>
<p>We make a closure to hold on to 3 variables. <code class="notranslate" translate="no">module</code>, <code class="notranslate" translate="no">sampler</code>, <code class="notranslate" translate="no">pipelineByFormat</code>.
For <code class="notranslate" translate="no">module</code> and <code class="notranslate" translate="no">sampler</code> we check if they have not be set and if not, we create a <code class="notranslate" translate="no">GPUSShaderModule</code>
and <a href="https://developer.mozilla.org/en-US/docs/Web/API/GPUSampler"><code class="notranslate" translate="no">GPUSampler</code></a> which we can hold on to and use in the future.</p>
</li>
<li>
<p>We have a pair of shaders that are almost exactly the same as all the examples. The only difference
is this part</p>
<pre class="prettyprint showlinemods notranslate lang-wgsl" translate="no">-  vsOutput.position = uni.matrix * vec4f(xy, 0.0, 1.0);
-  vsOutput.texcoord = xy * vec2f(1, 50);
+  vsOutput.position = vec4f(xy * 2.0 - 1.0, 0.0, 1.0);
+  vsOutput.texcoord = vec2f(xy.x, 1.0 - xy.y);
</pre>
<p>The hard coded quad position data we have in shader goes from 0.0 to 1.0 and so, as is, would only
cover the top right quarter texture we‚Äôre drawing to, just as it does in the examples. We need it to cover the entire
area so by multiplying by 2 and subtracting 1 we get a quad that goes from -1,-1 to +1,+1.</p>
<p>We also flip the Y texture coordinate. This is because when drawing to the texture +1, +1 is at the top right
but we want the top right of the texture we are sampling to be there. The top right of the sampled texture is +1, 0</p>
</li>
<li>
<p>We have an object, <code class="notranslate" translate="no">pipelineByFormat</code> which we use as a map of pipelines to texture formats.
This is because a pipeline needs to know the format to use.</p>
</li>
<li>
<p>We check if we already have a pipeline for a particular format and if not create one</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">    if (!pipelineByFormat[texture.format]) {
      pipelineByFormat[texture.format] = device.createRenderPipeline({
        label: 'mip level generator pipeline',
        layout: 'auto',
        vertex: {
          module,
        },
        fragment: {
          module,
+          targets: [{ format: texture.format }],
        },
      });
    }
    const pipeline = pipelineByFormat[texture.format];
</pre>
<p>The only major difference here is <code class="notranslate" translate="no">targets</code> is set from the texture‚Äôs format,
not from the <code class="notranslate" translate="no">presentationFormat</code> we use when rendering to the canvas</p>
</li>
<li>
<p>We finally use some parameters to <code class="notranslate" translate="no">texture.createView</code></p>
<p>We loop over each mip level. We create a bind group for the last mip with data in it
and we set the renderPassDescriptor to draw to the next mip level. Then we encode
a renderPass for that specific mip level. When we‚Äôre done. All the mips will have
been filled out.</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">    let width = texture.width;
    let height = texture.height;
    let baseMipLevel = 0;
    while (width &gt; 1 || height &gt; 1) {
      width = Math.max(1, width / 2 | 0);
      height = Math.max(1, height / 2 | 0);

      const bindGroup = device.createBindGroup({
        layout: pipeline.getBindGroupLayout(0),
        entries: [
          { binding: 0, resource: sampler },
+          { binding: 1, resource: texture.createView({baseMipLevel, mipLevelCount: 1}) },
        ],
      });

+      ++baseMipLevel;

      const renderPassDescriptor = {
        label: 'our basic canvas renderPass',
        colorAttachments: [
          {
+            view: texture.createView({baseMipLevel, mipLevelCount: 1}),
            loadOp: 'clear',
            storeOp: 'store',
          },
        ],
      };

      const pass = encoder.beginRenderPass(renderPassDescriptor);
      pass.setPipeline(pipeline);
      pass.setBindGroup(0, bindGroup);
      pass.draw(6);  // call our vertex shader 6 times
      pass.end();
    }

    const commandBuffer = encoder.finish();
    device.queue.submit([commandBuffer]);
</pre>
</li>
</ul>
<p>Let‚Äôs create some support functions make it simple load an image
into a texture and generate mips</p>
<p>Here‚Äôs a function that updates the first mip level and optionally flips the image.
If the image has mip levels then we generate them.</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">  function copySourceToTexture(device, texture, source, {flipY} = {}) {
    device.queue.copyExternalImageToTexture(
      { source, flipY, },
      { texture },
      { width: source.width, height: source.height },
    );

    if (texture.mipLevelCount &gt; 1) {
      generateMips(device, texture);
    }
  }
</pre>
<p><a id="a-create-texture-from-source"></a>Here‚Äôs a function that given a source (in this case an <a href="https://developer.mozilla.org/en-US/docs/Web/API/ImageBitmap"><code class="notranslate" translate="no">ImageBitmap</code></a>) will
create a texture of the matching size and then call the previous function
to fill it in with the data</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">  function createTextureFromSource(device, source, options = {}) {
    const texture = device.createTexture({
      format: 'rgba8unorm',
*      mipLevelCount: options.mips ? numMipLevels(source.width, source.height) : 1,
      size: [source.width, source.height],
      usage: GPUTextureUsage.TEXTURE_BINDING |
             GPUTextureUsage.COPY_DST |
             GPUTextureUsage.RENDER_ATTACHMENT,
    });
    copySourceToTexture(device, texture, source, options);
    return texture;
  }
</pre>
<p>and here‚Äôs a function that given a url will load the url as an <a href="https://developer.mozilla.org/en-US/docs/Web/API/ImageBitmap"><code class="notranslate" translate="no">ImageBitmap</code></a> call
call the previous function to create a texture and fill it with the contents of the image.</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">  async function createTextureFromImage(device, url, options) {
    const imgBitmap = await loadImageBitmap(url);
    return createTextureFromSource(device, imgBitmap, options);
  }
</pre>
<p>With those setup, the only major change to the <a href="webgpu-textures.html#a-mipmap-filter">mipmapFilter sample</a>
is this</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">-  const textures = [
-    createTextureWithMips(createBlendedMipmap(), 'blended'),
-    createTextureWithMips(createCheckedMipmap(), 'checker'),
-  ];
+  const textures = await Promise.all([
+    await createTextureFromImage(device,
+        'resources/images/f-texture.png', {mips: true, flipY: false}),
+    await createTextureFromImage(device,
+        'resources/images/coins.jpg', {mips: true}),
+    await createTextureFromImage(device,
+        'resources/images/Granite_paving_tileable_512x512.jpeg', {mips: true}),
+  ]);
</pre>
<p>The code above loads the F texture from above as well as these 2 tiling textures</p>
<div class="webgpu_center side-by-side">
  <div class="separate">
    <img src="../resources/images/coins.jpg">
    <div class="copyright">
      <a href="https://renderman.pixar.com/pixar-one-thirty">CC-BY: Pixar</a>
    </div>
  </div>
  <div class="separate">
    <img src="../resources/images/Granite_paving_tileable_512x512.jpeg">
    <div class="copyright">
       <a href="https://commons.wikimedia.org/wiki/File:Granite_paving_tileable_2048x2048.jpg">CC-BY-SA: Coyau</a>
    </div>
  </div>
</div>
<p>And here it is</p>
<p></p><div class="webgpu_example_container">
  <div><iframe class="webgpu_example" style=" " src="/webgpu/resources/editor.html?url=/webgpu/lessons/..%2Fwebgpu-simple-textured-quad-import.html"></iframe></div>
  <a class="webgpu_center" href="/webgpu/lessons/../webgpu-simple-textured-quad-import.html" target="_blank">click here to open in a separate window</a>
</div>

<p></p>
<h2 id="loading-canvas"><a id="a-loading-canvas"></a> Loading Canvas</h2>
<p><code class="notranslate" translate="no">copyExternalImageToTexture</code> takes other <em>sources</em>. Another is an <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLCanvasElement"><code class="notranslate" translate="no">HTMLCanvasElement</code></a>.
We can use this to draw things in a 2d canvas, and then get the result in a texture in WebGPU.
Of course you can use WebGPU to draw to a texture and use that texture you just drew to
in something else you render. In fact we just did that, rendering to a mip level and then
using that mip level a texture attachment to render to the next mip level.</p>
<p>But, sometimes using 2d canvas can make certain things easy. The 2d canvas has relatively
high level API.</p>
<p>So, first let‚Äôs make some kind of canvas animation.</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">const size = 256;
const half = size / 2;

const ctx = document.createElement('canvas').getContext('2d');
ctx.canvas.width = size;
ctx.canvas.height = size;

const hsl = (h, s, l) =&gt; `hsl(${h * 360 | 0}, ${s * 100}%, ${l * 100 | 0}%)`;

function update2DCanvas(time) {
  time *= 0.0001;
  ctx.clearRect(0, 0, size, size);
  ctx.save();
  ctx.translate(half, half);
  const num = 20;
  for (let i = 0; i &lt; num; ++i) {
    ctx.fillStyle = hsl(i / num * 0.2 + time * 0.1, 1, i % 2 * 0.5);
    ctx.fillRect(-half, -half, size, size);
    ctx.rotate(time * 0.5);
    ctx.scale(0.85, 0.85);
    ctx.translate(size / 16, 0);
  }
  ctx.restore();
}

function render(time) {
  update2DCanvas(time);
  requestAnimationFrame(render);
}
requestAnimationFrame(render);
</pre>
<p></p><div class="webgpu_example_container">
  <div><iframe class="webgpu_example" style=" " src="/webgpu/resources/editor.html?url=/webgpu/lessons/..%2Fcanvas-2d-animation.html"></iframe></div>
  <a class="webgpu_center" href="/webgpu/lessons/../canvas-2d-animation.html" target="_blank">click here to open in a separate window</a>
</div>

<p></p>
<p>To load that canvas into WebGPU only a few changes are needed to our previous example.</p>
<p>We need to create a texture of the right size. The easiest way it just to use the same
code we wrote above</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">+  const texture = createTextureFromSource(device, ctx.canvas, {mips: true});

  const textures = await Promise.all([
-    await createTextureFromImage(device,
-        'resources/images/f-texture.png', {mips: true, flipY: false}),
-    await createTextureFromImage(device,
-        'resources/images/coins.jpg', {mips: true}),
-    await createTextureFromImage(device,
-        'resources/images/Granite_paving_tileable_512x512.jpeg', {mips: true}),
+    texture,
  ]);
</pre>
<p>Then we need to switch to a <code class="notranslate" translate="no">requestAnimationFrame</code> loop, update the 2D canvas, and
then upload it to WebGPU</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">-  function render() {
+  function render(time) {
+    update2DCanvas(time);
+    copySourceToTexture(device, texture, ctx.canvas);

     ...


    requestAnimationFrame(render);
  }
  requestAnimationFrame(render);

  const observer = new ResizeObserver(entries =&gt; {
    for (const entry of entries) {
      const canvas = entry.target;
      const width = entry.contentBoxSize[0].inlineSize;
      const height = entry.contentBoxSize[0].blockSize;
      canvas.width = Math.max(1, Math.min(width, device.limits.maxTextureDimension2D));
      canvas.height = Math.max(1, Math.min(height, device.limits.maxTextureDimension2D));
-      render();
    }
  });
  observer.observe(canvas);

  canvas.addEventListener('click', () =&gt; {
    texNdx = (texNdx + 1) % textures.length;
-    render();
  });
</pre>
<p>With that we‚Äôre able to upload a canvas AND generate mips levels for it</p>
<p></p><div class="webgpu_example_container">
  <div><iframe class="webgpu_example" style=" " src="/webgpu/resources/editor.html?url=/webgpu/lessons/..%2Fwebgpu-simple-textured-quad-import-canvas.html"></iframe></div>
  <a class="webgpu_center" href="/webgpu/lessons/../webgpu-simple-textured-quad-import-canvas.html" target="_blank">click here to open in a separate window</a>
</div>

<p></p>
<h2 id="loading-video"><a id="a-loading-video"></a> Loading Video</h2>
<p>Loading video this way is no different. We can create a <code class="notranslate" translate="no">&lt;video&gt;</code> element and pass
it to the same functions we passed the canvas to in the previous example and it should
just work with minor adjustments</p>
<p>Here‚Äôs a video</p>
<div class="webgpu_center">
  <div>
     <video muted="" controls="" src="../resources/videos/Golden_retriever_swimming_the_doggy_paddle-360-no-audio.webm" style="width: 720px" ;=""></video>
     <div class="copyright"><a href="https://commons.wikimedia.org/wiki/File:Golden_retriever_swimming_the_doggy_paddle.webm">CC-BY: Golden Woofs</a></div>
  </div>
</div>
<p><a href="https://developer.mozilla.org/en-US/docs/Web/API/ImageBitmap"><code class="notranslate" translate="no">ImageBitmap</code></a> and <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLCanvasElement"><code class="notranslate" translate="no">HTMLCanvasElement</code></a> have their width and height as <code class="notranslate" translate="no">width</code> and <code class="notranslate" translate="no">height</code> properties but <a href="https://developer.mozilla.org/en-US/docs/Web/API/HTMLVideoElement"><code class="notranslate" translate="no">HTMLVideoElement</code></a> has its width and height
on <code class="notranslate" translate="no">videoWidth</code> and <code class="notranslate" translate="no">videoHeight</code>. So, let‚Äôs update the code to handle that difference</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">+  function getSourceSize(source) {
+    return [
+      source.videoWidth || source.width,
+      source.videoHeight || source.height,
+    ];
+  }

  function copySourceToTexture(device, texture, source, {flipY} = {}) {
    device.queue.copyExternalImageToTexture(
      { source, flipY, },
      { texture },
-      { width: source.width, height: source.height },
+      getSourceSize(source),
    );

    if (texture.mipLevelCount &gt; 1) {
      generateMips(device, texture);
    }
  }

  function createTextureFromSource(device, source, options = {}) {
+    const size = getSourceSize(source);
    const texture = device.createTexture({
      format: 'rgba8unorm',
-      mipLevelCount: options.mips ? numMipLevels(source.width, source.height) : 1,
-      size: [source.width, source.height],
+      mipLevelCount: options.mips ? numMipLevels(...size) : 1,
+      size,
      usage: GPUTextureUsage.TEXTURE_BINDING |
             GPUTextureUsage.COPY_DST |
             GPUTextureUsage.RENDER_ATTACHMENT,
    });
    copySourceToTexture(device, texture, source, options);
    return texture;
  }
</pre>
<p>So then, lets setup a video element</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">  const video = document.createElement('video');
  video.muted = true;
  video.loop = true;
  video.preload = 'auto';
  video.src = 'resources/videos/Golden_retriever_swimming_the_doggy_paddle-360-no-audio.webm';

  const texture = createTextureFromSource(device, video, {mips: true});
</pre>
<p>and update it at render time</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">-  function render(time) {
-    update2DCanvas(time);
-    copySourceToTexture(device, texture, ctx.canvas);
+  function render() {
+    copySourceToTexture(device, texture, video);
</pre>
<p>One complication of videos is we need to wait for them to have started
playing before we pass them to WebGPU. In modern browsers we can do
this by calling <code class="notranslate" translate="no">video.requestVideoFrameCallback</code>. It calls us each time
a new frame is available so we can use it to find out when at least
one frame is available.</p>
<p>For a fallback, we can wait for the time to advance and pray üôè because
sadly, old browsers made it hard to know when it‚Äôs safe to use a video üòÖ</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">+  function startPlayingAndWaitForVideo(video) {
+    return new Promise((resolve, reject) =&gt; {
+      video.addEventListener('error', reject);
+      if ('requestVideoFrameCallback' in video) {
+        video.requestVideoFrameCallback(resolve);
+      } else {
+        const timeWatcher = () =&gt; {
+          if (video.currentTime &gt; 0) {
+            resolve();
+          } else {
+            requestAnimationFrame(timeWatcher);
+          }
+        };
+        timeWatcher();
+      }
+      video.play().catch(reject);
+    });
+  }

  const video = document.createElement('video');
  video.muted = true;
  video.loop = true;
  video.preload = 'auto';
  video.src = 'resources/videos/Golden_retriever_swimming_the_doggy_paddle-360-no-audio.webm';
+  await startPlayingAndWaitForVideo(video);

  const texture = createTextureFromSource(device, video, {mips: true});
</pre>
<p>Another complication is we need to wait for the user to interact with the
page before we can start the video <sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>. Let‚Äôs add some HTML with
a play button.</p>
<pre class="prettyprint showlinemods notranslate lang-html" translate="no">  &lt;body&gt;
    &lt;canvas&gt;&lt;/canvas&gt;
+    &lt;div id="start"&gt;
+      &lt;div&gt;‚ñ∂Ô∏è&lt;/div&gt;
+    &lt;/div&gt;
  &lt;/body&gt;
</pre>
<p>And some CSS to center it</p>
<pre class="prettyprint showlinemods notranslate lang-css" translate="no">#start {
  position: fixed;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  display: flex;
  justify-content: center;
  align-items: center;
}
#start&gt;div {
  font-size: 200px;
  cursor: pointer;
}
</pre>
<p>Then let‚Äôs write a function to wait for it to be clicked and hide it.</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">+  function waitForClick() {
+    return new Promise(resolve =&gt; {
+      window.addEventListener(
+        'click',
+        () =&gt; {
+          document.querySelector('#start').style.display = 'none';
+          resolve();
+        },
+        { once: true });
+    });
+  }

  const video = document.createElement('video');
  video.muted = true;
  video.loop = true;
  video.preload = 'auto';
  video.src = 'resources/videos/Golden_retriever_swimming_the_doggy_paddle-360-no-audio.webm';
+  await waitForClick();
  await startPlayingAndWaitForVideo(video);

  const texture = createTextureFromSource(device, video, {mips: true});
</pre>
<p>Let‚Äôs also add a wait to pause the video</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">  const video = document.createElement('video');
  video.muted = true;
  video.loop = true;
  video.preload = 'auto';
  video.src = 'resources/videos/pexels-anna-bondarenko-5534310 (540p).mp4'; /* webgpufundamentals: url */
  await waitForClick();
  await startPlayingAndWaitForVideo(video);

+  canvas.addEventListener('click', () =&gt; {
+    if (video.paused) {
+      video.play();
+    } else {
+      video.pause();
+    }
+  });
</pre>
<p>And with that we should get video in a texture</p>
<p></p><div class="webgpu_example_container">
  <div><iframe class="webgpu_example" style=" " src="/webgpu/resources/editor.html?url=/webgpu/lessons/..%2Fwebgpu-simple-textured-quad-import-video.html"></iframe></div>
  <a class="webgpu_center" href="/webgpu/lessons/../webgpu-simple-textured-quad-import-video.html" target="_blank">click here to open in a separate window</a>
</div>

<p></p>
<p>One optimization we could make. We could only update the texture when
the video has changed.</p>
<p>For example</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">  const video = document.createElement('video');
  video.muted = true;
  video.loop = true;
  video.preload = 'auto';
  video.src = 'resources/videos/Golden_retriever_swimming_the_doggy_paddle-360-no-audio.webm';
  await waitForClick();
  await startPlayingAndWaitForVideo(video);

+  let alwaysUpdateVideo = !('requestVideoFrameCallback' in video);
+  let haveNewVideoFrame = false;
+  if (!alwaysUpdateVideo) {
+    function recordHaveNewFrame() {
+      haveNewVideoFrame = true;
+      video.requestVideoFrameCallback(recordHaveNewFrame);
+    }
+    video.requestVideoFrameCallback(recordHaveNewFrame);
+  }

  ...

  function render() {
+    if (alwaysUpdateVideo || haveNewVideoFrame) {
+      haveNewVideoFrame = false;
      copySourceToTexture(device, texture, video);
+    }

    ...
</pre>
<p>With this change we‚Äôd only update the video for each new frame. So, for example, on a device
with a display rate of 120 frames per second we‚Äôd draw at 120 frames per second so animations,
camera movements, etc would be smooth. But, the video texture itself would only update at its own frame
rate (for example 30fps).</p>
<p><strong>BUT! WebGPU has special support for using video efficiently</strong></p>
<p>We‚Äôll cover that in <a href="webgpu-textures-external-video.html">another article</a>.
The way above, using <code class="notranslate" translate="no">device.query.copyExternalImageToTexture</code> is actually
making <strong>a copy</strong>. Making a copy takes time. For example a 4k video‚Äôs resolution
is generally 3840 √ó 2160 which for <code class="notranslate" translate="no">rgba8unorm</code> is 31meg of data that needs to be
copied, <strong>per frame</strong>. <a href="webgpu-textures-external-video.html">External textures</a>
let you use the video‚Äôs data directly (no copy) but require different methods
and have some restrictions.</p>
<h2 id="texture-atlases"><a id="a-texture-atlases"></a> Texture Atlases</h2>
<p>From the examples above, we can see that to draw something with a texture
we have to create the texture, put data it in, bind it to bindGroup with
a sampler,
and reference it from a shader. So what would we do if we wanted
to draw multiple different textures on an object? Say we had a chair where the legs and back
are made of wood but the cushion is made of cloth.</p>
<div class="webgpu_center">
  <div class="center">
    <model-viewer src="../resources/models/gltf/cc0_chair.glb" camera-controls="" touch-action="pan-y" camera-orbit="45deg 70deg 2.5m" interaction-prompt="none" disable-zoom="" disable-pan="" style="width: 400px; height: 400px;"></model-viewer>
  </div>
  <div>
    <a href="https://skfb.ly/opnwY"></a>"[CC0] Chair" by adadadad5252341 <a href="http://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a>
  </div>
</div>
<p>Or a car where the tires are rubber, the body is paint, the bumpers and hub caps
are chrome.</p>
<div class="webgpu_center">
  <div class="center">
    <model-viewer src="../resources/models/gltf/classic_muscle_car.glb" camera-controls="" touch-action="pan-y" camera-orbit="45deg 70deg 20m" interaction-prompt="none" disable-zoom="" disable-pan="" style="width: 700px; height: 400px;"></model-viewer>
  </div>
  <div>
    <a href="https://skfb.ly/6Usqo"></a>"Classic Muscle car" by Lexyc16 <a href="http://creativecommons.org/licenses/by/4.0/">CC-BY 4.0</a>
  </div>
</div>
<p>If we did nothing else you might think we‚Äôd have to draw 2 times for
the chair, once for the wood with a wood texture, and once for the
cushion with a cloth texture. For the car we‚Äôd have several draws, one for
the tires, one for the body, one for the bumpers, etc‚Ä¶</p>
<p>That would end up being slow as every object would require multiple
draw calls. We could try to fix that by adding more inputs to our
shader (2, 3, 4 textures) with texture coordinates for each one
but that would not be very flexible and would be slow as well
as we‚Äôd need to read all 4 textures and add code to chose between them.</p>
<p>The most common way to cover this case is to use what‚Äôs called a
<a href="https://www.google.com/search?q=texture+atlas">Texture Atlas</a>.
A Texture Atlas is a fancy name for a texture with
multiple images it in. We then use texture coordinates to select
which parts go where.</p>
<p>Let‚Äôs wrap a cube with these 6 images</p>
<div class="webgpu_table_div_center">
  <style>
    table.webgpu_table_center {
      border-spacing: 0.5em;
      border-collapse: separate;
    }
    table.webgpu_table_center img {
      display:block;
    }
  </style>
  <table class="webgpu_table_center">
    <tbody><tr><td><img src="resources/noodles-01.jpg"></td><td><img src="resources/noodles-02.jpg"></td></tr>
    <tr><td><img src="resources/noodles-03.jpg"></td><td><img src="resources/noodles-04.jpg"></td></tr>
    <tr><td><img src="resources/noodles-05.jpg"></td><td><img src="resources/noodles-06.jpg"></td></tr>
  </tbody></table>
</div>
<p>Using some image editing software like Photoshop or <a href="https://photopea.com">Photopea</a> we could put all 6 images into a single image</p>
<img class="webgpu_center" src="../resources/images/noodles.jpg">
<p>We‚Äôd then make a cube and provide texture coordinates that select each
portion of the image onto a specific face of the cube. To keep it simple I put
all 6 images in the texture above in squares, 4x2. So it should be pretty
easy to compute the texture coordinates for each square.</p>
<div class="webgpu_center center diagram">
  <div>
    <div data-diagram="texture-atlas" style="display: inline-block; width: 600px;"></div>
  </div>
</div>
<blockquote>
<p>The diagram above might be confusing because it is often suggested that texture coordinates
have 0,0 as the bottom left corner. Really though there is no ‚Äúbottom‚Äù. There is just the idea
that texture coordinate 0,0 references the first pixel in the texture‚Äôs data. The first
pixel in the texture‚Äôs data is the top left corner of the image.
If you subscribe to the idea that 0,0 = bottom left then our texture coordinates
would be visualized like this. <strong>They‚Äôre still the same coordinates</strong>.</p>
</blockquote>
<div class="webgpu_center center diagram">
  <div>
    <div data-diagram="texture-atlas-bottom-left" style="display: inline-block; width: 600px;"></div>
    <div class="center">0,0 at bottom left</div>
  </div>
</div>
<p>Here‚Äôs the position vertices for a cube and the texture coordinates
to go with them</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">function createCubeVertices() {
  const vertexData = new Float32Array([
     //  position   |  texture coordinate
     //-------------+----------------------
     // front face     select the top left image
    -1,  1,  1,        0   , 0  ,
    -1, -1,  1,        0   , 0.5,
     1,  1,  1,        0.25, 0  ,
     1, -1,  1,        0.25, 0.5,
     // right face     select the top middle image
     1,  1, -1,        0.25, 0  ,
     1,  1,  1,        0.5 , 0  ,
     1, -1, -1,        0.25, 0.5,
     1, -1,  1,        0.5 , 0.5,
     // back face      select to top right image
     1,  1, -1,        0.5 , 0  ,
     1, -1, -1,        0.5 , 0.5,
    -1,  1, -1,        0.75, 0  ,
    -1, -1, -1,        0.75, 0.5,
    // left face       select the bottom left image
    -1,  1,  1,        0   , 0.5,
    -1,  1, -1,        0.25, 0.5,
    -1, -1,  1,        0   , 1  ,
    -1, -1, -1,        0.25, 1  ,
    // bottom face     select the bottom middle image
     1, -1,  1,        0.25, 0.5,
    -1, -1,  1,        0.5 , 0.5,
     1, -1, -1,        0.25, 1  ,
    -1, -1, -1,        0.5 , 1  ,
    // top face        select the bottom right image
    -1,  1,  1,        0.5 , 0.5,
     1,  1,  1,        0.75, 0.5,
    -1,  1, -1,        0.5 , 1  ,
     1,  1, -1,        0.75, 1  ,

  ]);

  const indexData = new Uint16Array([
     0,  1,  2,  2,  1,  3,  // front
     4,  5,  6,  6,  5,  7,  // right
     8,  9, 10, 10,  9, 11,  // back
    12, 13, 14, 14, 13, 15,  // left
    16, 17, 18, 18, 17, 19,  // bottom
    20, 21, 22, 22, 21, 23,  // top
  ]);

  return {
    vertexData,
    indexData,
    numVertices: indexData.length,
  };
}
</pre>
<p>To make this example we‚Äôre going to have start with an example from <a href="webgpu-cameras.html">the article on cameras</a>.
If you haven‚Äôt read the article yet you can read it and the series it‚Äôs a part of the learn how do 3D.
For now, the important part is, like we did above, we output positions and texture coordinates from our
vertex shader and use them to look up values from a texture in our fragment shader. So, here‚Äôs
the changes needed from the shader in the camera example, applying what we have above.</p>
<pre class="prettyprint showlinemods notranslate lang-wgsl" translate="no">struct Uniforms {
  matrix: mat4x4f,
};

struct Vertex {
  @location(0) position: vec4f,
-  @location(1) color: vec4f,
+  @location(1) texcoord: vec2f,
};

struct VSOutput {
  @builtin(position) position: vec4f,
-  @location(0) color: vec4f,
+  @location(0) texcoord: vec2f,
};

@group(0) @binding(0) var&lt;uniform&gt; uni: Uniforms;
+@group(0) @binding(1) var ourSampler: sampler;
+@group(0) @binding(2) var ourTexture: texture_2d&lt;f32&gt;;

@vertex fn vs(vert: Vertex) -&gt; VSOutput {
  var vsOut: VSOutput;
  vsOut.position = uni.matrix * vert.position;
-  vsOut.color = vert.color;
+  vsOut.texcoord = vert.texcoord;
  return vsOut;
}

@fragment fn fs(vsOut: VSOutput) -&gt; @location(0) vec4f {
-  return vsOut.color;
+  return textureSample(ourTexture, ourSampler, vsOut.texcoord);
}
</pre>
<p>All we did was switch from taking a color per vertex to a texture coordinate per vertex
and passing that texture coordinate to the fragment shader, like we did above. We then
use it, in the fragment shader, like we did above.</p>
<p>In JavaScript we need to change that example‚Äôs pipeline from taking a color to taking
texture coordinates</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">  const pipeline = device.createRenderPipeline({
    label: '2 attributes',
    layout: 'auto',
    vertex: {
      module,
      buffers: [
        {
-          arrayStride: (4) * 4, // (3) floats 4 bytes each + one 4 byte color
+          arrayStride: (3 + 2) * 4, // (3+2) floats 4 bytes each
          attributes: [
            {shaderLocation: 0, offset: 0, format: 'float32x3'},  // position
-            {shaderLocation: 1, offset: 12, format: 'unorm8x4'},  // color
+            {shaderLocation: 1, offset: 12, format: 'float32x2'},  // texcoord
          ],
        },
      ],
    },
    fragment: {
      module,
      targets: [{ format: presentationFormat }],
    },
    primitive: {
      cullMode: 'back',
    },
    depthStencil: {
      depthWriteEnabled: true,
      depthCompare: 'less',
      format: 'depth24plus',
    },
  });
</pre>
<p>To keep the data smaller we‚Äôre going to use indices like we covered in <a href="webgpu-vertex-buffers.html">the article on vertex buffers</a>.</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">-  const { vertexData, numVertices } = createFVertices();
+  const { vertexData, indexData, numVertices } = createCubeVertices();
  const vertexBuffer = device.createBuffer({
    label: 'vertex buffer vertices',
    size: vertexData.byteLength,
    usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST,
  });
  device.queue.writeBuffer(vertexBuffer, 0, vertexData);

+  const indexBuffer = device.createBuffer({
+    label: 'index buffer',
+    size: vertexData.byteLength,
+    usage: GPUBufferUsage.INDEX | GPUBufferUsage.COPY_DST,
+  });
+  device.queue.writeBuffer(indexBuffer, 0, indexData);
</pre>
<p>We need to copy all of the texture loading and mip generation code into this example
and then use it to load the texture atlas image. We also need to make a sampler
and add them our bindGroup</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">+  const texture = await createTextureFromImage(device,
+      'resources/images/noodles.jpg', {mips: true, flipY: false});
+
+  const sampler = device.createSampler({
+    magFilter: 'linear',
+    minFilter: 'linear',
+    mipmapFilter: 'linear',
+  });

  const bindGroup = device.createBindGroup({
    label: 'bind group for object',
    layout: pipeline.getBindGroupLayout(0),
    entries: [
      { binding: 0, resource: { buffer: uniformBuffer }},
+      { binding: 1, resource: sampler },
+      { binding: 2, resource: texture.createView() },
    ],
  });
</pre>
<p>We need to do some 3D math to setup a matrix for drawing in 3D. (Again, see <a href="webgpu-cameras.html">the camera article</a> for
details on 3D math.)</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">  const degToRad = d =&gt; d * Math.PI / 180;

  const settings = {
    rotation: [degToRad(20), degToRad(25), degToRad(0)],
  };

  const radToDegOptions = { min: -360, max: 360, step: 1, converters: GUI.converters.radToDeg };

  const gui = new GUI();
  gui.onChange(render);
  gui.add(settings.rotation, '0', radToDegOptions).name('rotation.x');
  gui.add(settings.rotation, '1', radToDegOptions).name('rotation.y');
  gui.add(settings.rotation, '2', radToDegOptions).name('rotation.z');

  ...

  function render() {

    ...

    const aspect = canvas.clientWidth / canvas.clientHeight;
    mat4.perspective(
        60 * Math.PI / 180,
        aspect,
        0.1,      // zNear
        10,      // zFar
        matrixValue,
    );
    const view = mat4.lookAt(
      [0, 1, 5],  // camera position
      [0, 0, 0],  // target
      [0, 1, 0],  // up
    );
    mat4.multiply(matrixValue, view, matrixValue);
    mat4.rotateX(matrixValue, settings.rotation[0], matrixValue);
    mat4.rotateY(matrixValue, settings.rotation[1], matrixValue);
    mat4.rotateZ(matrixValue, settings.rotation[2], matrixValue);

    // upload the uniform values to the uniform buffer
    device.queue.writeBuffer(uniformBuffer, 0, uniformValues);
</pre>
<p>And at render time we need to draw with indices</p>
<pre class="prettyprint showlinemods notranslate lang-js" translate="no">    const encoder = device.createCommandEncoder();
    const pass = encoder.beginRenderPass(renderPassDescriptor);
    pass.setPipeline(pipeline);
    pass.setVertexBuffer(0, vertexBuffer);
+    pass.setIndexBuffer(indexBuffer, 'uint16');

    ...

    pass.setBindGroup(0, bindGroup);
-    pass.draw(numVertices);
+    pass.drawIndexed(numVertices);

    pass.end();
</pre>
<p>And we get a cube, with a different image on each side, using a single texture.</p>
<p></p><div class="webgpu_example_container">
  <div><iframe class="webgpu_example" style=" " src="/webgpu/resources/editor.html?url=/webgpu/lessons/..%2Fwebgpu-texture-atlas.html"></iframe></div>
  <a class="webgpu_center" href="/webgpu/lessons/../webgpu-texture-atlas.html" target="_blank">click here to open in a separate window</a>
</div>

<p></p>
<p>Using a texture atlas is good because there‚Äôs just 1 texture to load, the shader stays simple as it only has to reference 1 texture, and it only
requires 1 draw call to draw the shape instead of 1 draw call per texture as it might if we keep the images separate.</p>
<!-- keep this at the bottom of the article -->
<script type="module" src="/3rdparty/model-viewer.3.3.0.min.js"></script>
<script type="module" src="webgpu-importing-textures.js"></script>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>There are various ways to get a video, usually without audio,
to autoplay without having to wait for the user to interact with the page.
They seem to change over time so we won‚Äôt go into solutions here. <a href="#fnref1" class="footnote-backref">‚Ü©Ô∏é</a></p>
</li>
</ol>
</section>

    </div>
    <div class="lesson-sidebar">
        <select class="language">
    <option value="/webgpu/lessons/webgpu-importing-textures.html" selected="">English
    </option><option value="/webgpu/lessons/es/webgpu-importing-textures.html">Spanish
    </option><option value="/webgpu/lessons/ja/webgpu-importing-textures.html">Êó•Êú¨Ë™û
    </option><option value="/webgpu/lessons/ko/webgpu-importing-textures.html">ÌïúÍµ≠Ïñ¥
    </option><option value="/webgpu/lessons/ru/webgpu-importing-textures.html">–†—É—Å—Å–∫–∏–π
    </option><option value="/webgpu/lessons/uk/webgpu-importing-textures.html">–£–∫—Ä–∞—ó–Ω—Å—å–∫–∞
    </option><option value="/webgpu/lessons/zh_cn/webgpu-importing-textures.html">ÁÆÄ‰Ωì‰∏≠Êñá
</option></select>


        <div id="toc">
          <ul>  <li>Basics</li>
        <ul>
          <li><a href="/webgpu/lessons/webgpu-fundamentals.html">Fundamentals</a></li>
<li><a href="/webgpu/lessons/webgpu-inter-stage-variables.html">Inter-stage Variables</a></li>
<li><a href="/webgpu/lessons/webgpu-uniforms.html">Uniforms</a></li>
<li><a href="/webgpu/lessons/webgpu-storage-buffers.html">Storage Buffers</a></li>
<li><a href="/webgpu/lessons/webgpu-vertex-buffers.html">Vertex Buffers</a></li>
  <li>Textures</li>
        <ul>
          <li><a href="/webgpu/lessons/webgpu-textures.html">Textures</a></li>
<li><a href="/webgpu/lessons/webgpu-importing-textures.html">Loading Images</a></li>
<li><a href="/webgpu/lessons/webgpu-textures-external-video.html">Using Video</a></li>
<li><a href="/webgpu/lessons/webgpu-cube-maps.html">Cube Maps</a></li>
<li><a href="/webgpu/lessons/webgpu-storage-textures.html">Storage Textures</a></li>
<li><a href="/webgpu/lessons/webgpu-multisampling.html">Multisampling / MSAA</a></li>
        </ul>
<li><a href="/webgpu/lessons/webgpu-constants.html">Constants</a></li>
<li><a href="/webgpu/lessons/webgpu-memory-layout.html">Data Memory Layout</a></li>
<li><a href="/webgpu/lessons/webgpu-transparency.html">Transparency and Blending</a></li>
<li><a href="/webgpu/lessons/webgpu-copying-data.html">Copying Data</a></li>
<li><a href="/webgpu/lessons/webgpu-limits-and-features.html">Optional Features and Limits</a></li>
<li><a href="/webgpu/lessons/webgpu-timing.html">Timing Performance</a></li>
<li><a href="/webgpu/lessons/webgpu-wgsl.html">WGSL</a></li>
<li><a href="/webgpu/lessons/webgpu-how-it-works.html">How It Works</a></li>
        </ul>
  <li>3D Math</li>
        <ul>
          <li><a href="/webgpu/lessons/webgpu-translation.html">Translation</a></li>
<li><a href="/webgpu/lessons/webgpu-rotation.html">Rotation</a></li>
<li><a href="/webgpu/lessons/webgpu-scale.html">Scale</a></li>
<li><a href="/webgpu/lessons/webgpu-matrix-math.html">Matrix Math</a></li>
<li><a href="/webgpu/lessons/webgpu-orthographic-projection.html">Orthographic Projection</a></li>
<li><a href="/webgpu/lessons/webgpu-perspective-projection.html">Perspective Projection</a></li>
<li><a href="/webgpu/lessons/webgpu-cameras.html">Cameras</a></li>
<li><a href="/webgpu/lessons/webgpu-matrix-stacks.html">Matrix Stacks</a></li>
<li><a href="/webgpu/lessons/webgpu-scene-graphs.html">Scene Graphs</a></li>
        </ul>
  <li>Lighting</li>
        <ul>
          <li><a href="/webgpu/lessons/webgpu-lighting-directional.html">Directional Lighting</a></li>
<li><a href="/webgpu/lessons/webgpu-lighting-point.html">Point Lighting</a></li>
<li><a href="/webgpu/lessons/webgpu-lighting-spot.html">Spot Lighting</a></li>
        </ul>
  <li>Techniques</li>
        <ul>
            <li>2D</li>
        <ul>
          <li><a href="/webgpu/lessons/webgpu-large-triangle-to-cover-clip-space.html">Large Clip Space Triangle</a></li>
        </ul>
  <li>3D</li>
        <ul>
          <li><a href="/webgpu/lessons/webgpu-environment-maps.html">Environment maps</a></li>
<li><a href="/webgpu/lessons/webgpu-skybox.html">Skyboxes</a></li>
        </ul>
        </ul>
  <li>Compute Shaders</li>
        <ul>
          <li><a href="/webgpu/lessons/webgpu-compute-shaders.html">Compute Shader Basics</a></li>
<li><a href="/webgpu/lessons/webgpu-compute-shaders-histogram.html">Image Histogram</a></li>
<li><a href="/webgpu/lessons/webgpu-compute-shaders-histogram-part-2.html">Image Histogram Part 2</a></li>
        </ul>
  <li>Misc</li>
        <ul>
          <li><a href="/webgpu/lessons/webgpu-resizing-the-canvas.html">Resizing the Canvas</a></li>
<li><a href="/webgpu/lessons/webgpu-multiple-canvases.html">Multiple Canvases</a></li>
<li><a href="/webgpu/lessons/webgpu-points.html">Points</a></li>
<li><a href="/webgpu/lessons/webgpu-from-webgl.html">WebGPU from WebGL</a></li>
<li><a href="/webgpu/lessons/webgpu-resources.html">Resources / References</a></li>
<li><a href="/webgpu/lessons/webgpu-wgsl-function-reference.html">WGSL Function Reference</a></li>
<li><a href="/webgpu/lessons/resources/wgsl-offset-computer.html">WGSL Offset Computer</a></li>
        </ul></ul>
<ul>
  <li><a href="https://github.com/webgpu/webgpufundamentals">github</a></li>
  <li><a href="https://google.github.io/tour-of-wgsl/">Tour of WGSL</a></li>
  <li><a href="https://gpuweb.github.io/types/">WebGPU API Reference</a></li>
  <li><a href="https://www.w3.org/TR/webgpu/">WebGPU Spec</a></li>
  <li><a href="https://www.w3.org/TR/WGSL/">WGSL Spec</a></li>
  <li><a href="https://webgpureport.org">WebGPUReport.org</a></li>
  <li><a href="https://web3dsurvey.com/webgpu">Web3DSurvey.com</a></li>
</ul>

        </div>
    </div>
    <div class="lesson-comments">
        
<div>Questions? <a href="http://stackoverflow.com/questions/tagged/webgpu">Ask on stackoverflow</a>.</div>
<div>
   <a href="https://github.com/webgpu/webgpufundamentals/issues/new?assignees=&amp;labels=suggested+topic&amp;template=suggest-topic.md&amp;title=%5BSUGGESTION%5D">Suggestion</a>?
   <a href="https://github.com/webgpu/webgpufundamentals/issues/new?assignees=&amp;labels=&amp;template=request.md&amp;title=">Request</a>?
   <a href="https://github.com/webgpu/webgpufundamentals/issues/new?assignees=&amp;labels=bug+%2F+issue&amp;template=bug-issue-report.md&amp;title=">Issue</a>?
   <a href="https://github.com/webgpu/webgpufundamentals/issues/new?assignees=&amp;labels=bug+%2F+issue&amp;template=bug-issue-report.md&amp;title=">Bug</a>?
</div>
<div class="lesson-comment-notes">
   Use <b>&lt;pre&gt;&lt;code&gt;</b>code goes here<b>&lt;/code&gt;&lt;/pre&gt;</b> for code blocks
</div>
  

        <div id="disqus_thread"></div>
        <script>
            var disqus_config = function () {
              this.page.url = `${window.location.origin}${window.location.pathname}`;
              this.page.identifier = `WebGPU Loading Images into Textures`;
            };
            (function() { // DON'T EDIT BELOW THIS LINE
                if (window.location.hostname.indexOf("webgpufundamentals.org") < 0) {
                    return;
                }
                var d = document, s = d.createElement('script');
                s.src = 'https://webgpufundamentals-org.disqus.com/embed.js';
                s.setAttribute('data-timestamp', +new Date());
                (d.head || d.body).appendChild(s);
            })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    </div>
  </div>
</div>

<script>
const settings = {
  contribTemplate: "Thank you <a href=\"${html_url}\"><img src=\"${avatar_url}\"> ${login}</a><br>for <a href=\"https://github.com/${owner}/${repo}/commits?author=${login}\">${contributions} contributions</a>",
  owner: "gfxfundamentals",
  repo: "webgpufundamentals",
};
</script>
<script src="/contributors.js"></script>
<script>if (typeof module === 'object') {window.module = module; module = undefined;}</script>
<script src="/3rdparty/jquery-3.3.1.slim.min.js"></script>
<script src="/webgpu/lessons/resources/prettify.js"></script>
<script src="/webgpu/lessons/resources/lesson.js" type="module"></script>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-92BFT5PE4H"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-92BFT5PE4H');
</script>






</body></html>