<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
    <title>WebGPU Post Processing - Step 9 - 3D-LUTs</title>
    <style>
      @import url(resources/webgpu-lesson.css);
html, body {
  margin: 0;       /* remove the default margin          */
  height: 100%;    /* make the html,body fill the page   */
}
canvas {
  display: block;  /* make the canvas act like a block   */
  width: 100%;     /* make the canvas fill its container */
  height: 100%;
}
    </style>
  </head>
  <body>
    <canvas></canvas>
  </body>
  <script type="module">
import GUI from '../3rdparty/muigui-0.x.module.js';
// see https://webgpufundamentals.org/webgpu/lessons/webgpu-utils.html#webgpu-utils
import {createTextureFromImage} from '../3rdparty/webgpu-utils-1.x.module.js';
// see https://webgpufundamentals.org/webgpu/lessons/webgpu-matrix-math.html
import {mat4} from '../3rdparty/wgpu-matrix.module.js';

import * as dragAndDrop from './resources/js/drag-and-drop.js';
import onPasteImage from './resources/js/on-paste-image.js';

function makeIdentityLutTexture(device) {
  const texture = device.createTexture({
    size: [2, 2, 2],
    dimension: '3d',
    format: 'rgba8unorm',
    usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST,
  });

  const identityLUT = new Uint8Array([
      0,   0,   0, 255,  // black
    255,   0,   0, 255,  // red
      0, 255,   0, 255,  // green
    255, 255,   0, 255,  // yellow
      0,   0, 255, 255,  // blue
    255,   0, 255, 255,  // magenta
      0, 255, 255, 255,  // cyan
    255, 255, 255, 255,  // white
  ]);

  device.queue.writeTexture(
    { texture },
    identityLUT,
    { bytesPerRow: 8, rowsPerImage: 2},
    [2, 2, 2],
  );

  return texture;
}

/**
 * create a LUT texture from an image URL. You must pass in the size of the LUT
 * It's assumed to be in the top left corner of the image.
 *
 * +---------+---------+---------+---------+---------+---------+---→
 * |         |         |         |         |         |         |
 * | layer 0 | layer 1 | layer 2 | layer 3 |   ...   | layer n |
 * |         |         |         |         |         |         |
 * +---------+---------+---------+---------+---------+---------+
 * |
 * ↓
 */
const createLUTTextureFromImage = (function() {
  const ctx = new OffscreenCanvas(1, 1).getContext('2d', { willReadFrequently: true });

  return async function createLUTTextureFromImage(device, url, lutSize) {
    const img = new Image();
    img.src = url;
    await img.decode();
    ctx.canvas.width = lutSize * lutSize;
    ctx.canvas.height = lutSize;
    ctx.drawImage(img, 0, 0);
    const imgData = ctx.getImageData(0, 0, lutSize * lutSize, lutSize);

    const texture = device.createTexture({
      size: [lutSize, lutSize, lutSize],
      dimension: '3d',
      format: 'rgba8unorm',
      usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST,
    });

    for (let z = 0; z < lutSize; ++z) {
      device.queue.writeTexture(
        { texture, origin: [0, 0, z] },
        imgData.data,
        { offset: z * lutSize * 4, bytesPerRow: imgData.width * 4 },
        [lutSize, lutSize],
      );
    }
    return texture;
  };
})();

async function main() {
  const adapter = await navigator.gpu?.requestAdapter();
  const device = await adapter?.requestDevice();
  if (!device) {
    fail('need a browser that supports WebGPU');
    return;
  }

  // Get a WebGPU context from the canvas and configure it
  const canvas = document.querySelector('canvas');
  const context = canvas.getContext('webgpu');
  const presentationFormat = navigator.gpu.getPreferredCanvasFormat();
  context.configure({
    device,
    format: presentationFormat,
  });

  const module = device.createShaderModule({
    code: `
      struct VSOutput {
        @builtin(position) position: vec4f,
        @location(0) texcoord: vec2f,
      };

      struct Uniforms {
        matrix: mat4x4f,
      };

      @group(0) @binding(0) var<uniform> uni: Uniforms;
      @group(0) @binding(1) var tex: texture_2d<f32>;
      @group(0) @binding(2) var smp: sampler;

      @vertex fn vs(@builtin(vertex_index) vNdx: u32) -> VSOutput {
        let positions = array(
          vec2f( 0,  0),
          vec2f( 1,  0),
          vec2f( 0,  1),
          vec2f( 0,  1),
          vec2f( 1,  0),
          vec2f( 1,  1),
        );
        let pos = positions[vNdx];
        return VSOutput(
          uni.matrix * vec4f(pos, 0, 1),
          pos,
        );
      }

      @fragment fn fs(fsInput: VSOutput) -> @location(0) vec4f {
        return textureSample(tex, smp, fsInput.texcoord);
      }
    `,
  });

  const pipeline = device.createRenderPipeline({
    label: 'textured unit quad',
    layout: 'auto',
    vertex: {
      module,
    },
    fragment: {
      module,
      targets: [{ format: 'rgba8unorm' }],
    },
  });

  const renderPassDescriptor = {
    label: 'our basic canvas renderPass',
    colorAttachments: [
      {
        // view: <- to be filled out when we render
        clearValue: [0.3, 0.3, 0.3, 1],
        loadOp: 'clear',
        storeOp: 'store',
      },
    ],
  };

  const imageUniformBuffer = device.createBuffer({
    size: 4 * 16,
    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,
  });

  let imageTexture = await createTextureFromImage(
    device,
    'resources/images/david-clode-clown-fish.jpg', /* webgpufundamentals: url */
  );

  const imageSampler = device.createSampler({
    minFilter: 'linear',
    magFilter: 'linear',
  });

  let imageBindGroup;
  function updateBindGroup() {
    imageBindGroup = device.createBindGroup({
      layout: pipeline.getBindGroupLayout(0),
      entries: [
        { binding: 0, resource: { buffer: imageUniformBuffer } },
        { binding: 1, resource: imageTexture.createView() },
        { binding: 2, resource: imageSampler },
      ],
    });
  }
  updateBindGroup();

  const postProcessModule = device.createShaderModule({
    code: `
      struct VSOutput {
        @builtin(position) position: vec4f,
        @location(0) texcoord: vec2f,
      };

      fn apply3DLUT(
          color: vec3f,
          lut: texture_3d<f32>,
          smp: sampler) -> vec3f {
        let size = vec3f(textureDimensions(lut, 0));
        let range = (size - 1) / size;
        let uvw = 0.5 / size + color * range;
        return textureSample(lut, smp, uvw).rgb;
      }

      @vertex fn vs(
        @builtin(vertex_index) vertexIndex : u32,
      ) -> VSOutput {
        var pos = array(
          vec2f(-1.0, -1.0),
          vec2f(-1.0,  3.0),
          vec2f( 3.0, -1.0),
        );

        var vsOutput: VSOutput;
        let xy = pos[vertexIndex];
        vsOutput.position = vec4f(xy, 0.0, 1.0);
        vsOutput.texcoord = xy * vec2f(0.5) + vec2f(0.5);
        return vsOutput;
      }

      struct Uniforms {
        lutAmount: f32,
      };

      @group(0) @binding(0) var postTexture2d: texture_2d<f32>;
      @group(0) @binding(1) var postSampler: sampler;
      @group(0) @binding(2) var<uniform> uni: Uniforms;
      @group(1) @binding(0) var lut: texture_3d<f32>;
      @group(1) @binding(1) var lutSampler: sampler;

      @fragment fn fs2d(fsInput: VSOutput) -> @location(0) vec4f {
        let color = textureSample(postTexture2d, postSampler, fsInput.texcoord);
        var rgb = color.rgb;
        rgb = mix(rgb, apply3DLUT(rgb, lut, lutSampler), uni.lutAmount);
        return vec4f(rgb, color.a);
      }
    `,
  });

  const postProcessPipeline = device.createRenderPipeline({
    layout: 'auto',
    vertex: { module: postProcessModule },
    fragment: {
      module: postProcessModule,
      targets: [ { format: presentationFormat }],
    },
  });

  const postProcessSampler = device.createSampler({
    minFilter: 'linear',
    magFilter: 'linear',
  });

  const postProcessRenderPassDescriptor = {
    label: 'post process render pass',
    colorAttachments: [
      { loadOp: 'clear', storeOp: 'store' },
    ],
  };

  const postProcessUniformBuffer = device.createBuffer({
    size: 80,
    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,
  });

  let renderTarget;
  let postProcessBindGroup;

  const lutNearestSampler = device.createSampler();
  const lutLinearSampler = device.createSampler({
    magFilter: 'linear',
    minFilter: 'linear',
  });

  function makeLutBindGroup(texture, sampler) {
    return device.createBindGroup({
      layout: postProcessPipeline.getBindGroupLayout(1),
      entries: [
        { binding: 0, resource: texture.createView() },
        { binding: 1, resource: sampler },
      ],
    });
  }

  const identityLutTexture = makeIdentityLutTexture(device);
  const lutBindGroups = [
    {
      name: 'identity',
      bindGroup: makeLutBindGroup(identityLutTexture, lutLinearSampler),
    },
    {
      name: 'identity (nearest)',
      bindGroup: makeLutBindGroup(identityLutTexture, lutNearestSampler),
    },
  ];

  const lutTextures = [
    { name: 'custom',          url: 'resources/images/lut/3d-lut-orange-to-green-s16.png'}, /* webgpufundamentals: url */
    { name: 'monochrome',      url: 'resources/images/lut/monochrome-s8.png' }, /* webgpufundamentals: url */
    { name: 'sepia',           url: 'resources/images/lut/sepia-s8.png' }, /* webgpufundamentals: url */
    { name: 'saturated',       url: 'resources/images/lut/saturated-s8.png', }, /* webgpufundamentals: url */
    { name: 'posterize',       url: 'resources/images/lut/posterize-s8n.png', }, /* webgpufundamentals: url */
    { name: 'posterize-3-rgb', url: 'resources/images/lut/posterize-3-rgb-s8n.png', }, /* webgpufundamentals: url */
    { name: 'posterize-3-lab', url: 'resources/images/lut/posterize-3-lab-s8n.png', }, /* webgpufundamentals: url */
    { name: 'posterize-4-lab', url: 'resources/images/lut/posterize-4-lab-s8n.png', }, /* webgpufundamentals: url */
    { name: 'posterize-more',  url: 'resources/images/lut/posterize-more-s8n.png', }, /* webgpufundamentals: url */
    { name: 'inverse',         url: 'resources/images/lut/inverse-s8.png', }, /* webgpufundamentals: url */
    { name: 'color negative',  url: 'resources/images/lut/color-negative-s8.png', }, /* webgpufundamentals: url */
    { name: 'funky contrast',  url: 'resources/images/lut/funky-contrast-s8.png', }, /* webgpufundamentals: url */
    { name: 'nightvision',     url: 'resources/images/lut/nightvision-s8.png', }, /* webgpufundamentals: url */
    { name: 'thermal',         url: 'resources/images/lut/thermal-s8.png', }, /* webgpufundamentals: url */
    { name: 'b/w',             url: 'resources/images/lut/black-white-s8n.png', }, /* webgpufundamentals: url */
    { name: 'hue +60',         url: 'resources/images/lut/hue-plus-60-s8.png', }, /* webgpufundamentals: url */
    { name: 'hue +180',        url: 'resources/images/lut/hue-plus-180-s8.png', }, /* webgpufundamentals: url */
    { name: 'hue -60',         url: 'resources/images/lut/hue-minus-60-s8.png', }, /* webgpufundamentals: url */
    { name: 'red to cyan',     url: 'resources/images/lut/red-to-cyan-s8.png' }, /* webgpufundamentals: url */
    { name: 'blues',           url: 'resources/images/lut/blues-s8.png' }, /* webgpufundamentals: url */
    { name: 'infrared',        url: 'resources/images/lut/infrared-s8.png' }, /* webgpufundamentals: url */
    { name: 'radioactive',     url: 'resources/images/lut/radioactive-s8.png' }, /* webgpufundamentals: url */
    { name: 'goolgey',         url: 'resources/images/lut/googley-s8.png' }, /* webgpufundamentals: url */
    { name: 'bgy',             url: 'resources/images/lut/bgy-s8.png' }, /* webgpufundamentals: url */
  ];

  lutBindGroups.push(...await Promise.all(lutTextures.map(async({name, url}) => {
    // assumes filename ends in '-s<num>[n]'
    // where <num> is the size of the 3DLUT cube
    // and [n] means 'no filtering' or 'nearest'
    //
    // examples:
    //    'foo-s16.png' = size:16, filter: true
    //    'bar-s8n.png' = size:8, filter: false
    const m = /-s(\d+)(n*)\.[^.]+$/.exec(url);
    const size = parseInt(m[1]);
    const filter = m[2] === '';

    const texture = await createLUTTextureFromImage(device, url, size);
    const sampler = filter
      ? lutLinearSampler
      : lutNearestSampler;
    return {name, bindGroup: makeLutBindGroup(texture, sampler)};
  })));

  function setupPostProcess(canvasTexture) {
    if (renderTarget?.width === canvasTexture.width &&
        renderTarget?.height === canvasTexture.height) {
      return;
    }

    renderTarget?.destroy();
    renderTarget = device.createTexture({
      size: canvasTexture,
      format: 'rgba8unorm',
      usage: GPUTextureUsage.RENDER_ATTACHMENT | GPUTextureUsage.TEXTURE_BINDING,
    });
    const renderTargetView = renderTarget.createView();
    renderPassDescriptor.colorAttachments[0].view = renderTargetView;

    postProcessBindGroup = device.createBindGroup({
      layout: postProcessPipeline.getBindGroupLayout(0),
      entries: [
        { binding: 0, resource: renderTargetView },
        { binding: 1, resource: postProcessSampler },
        { binding: 2, resource: { buffer: postProcessUniformBuffer } },
      ],
    });
  }

  function postProcess(encoder, srcTexture, dstTexture) {
    device.queue.writeBuffer(
      postProcessUniformBuffer,
      0,
      new Float32Array([
        settings.lutAmount,
      ]),
    );

    postProcessRenderPassDescriptor.colorAttachments[0].view = dstTexture.createView();
    const pass = encoder.beginRenderPass(postProcessRenderPassDescriptor);
    pass.setPipeline(postProcessPipeline);
    pass.setBindGroup(0, postProcessBindGroup);
    pass.setBindGroup(1, lutBindGroups[settings.lut].bindGroup);
    pass.draw(3);
    pass.end();
  }

  const settings = {
    lutAmount: 1,
    lut: 2,
  };

  const gui = new GUI();
  gui.onChange(render);
  gui.add(settings, 'lutAmount', 0, 1);
  const keyValues = Object.fromEntries(lutBindGroups.map(({name}, i) => [name, i]));
  gui.add(settings, 'lut', { keyValues });

  function render() {
    const canvasTexture = context.getCurrentTexture();
    setupPostProcess(canvasTexture);

    // css 'cover'
    const canvasAspect = canvas.clientWidth / canvas.clientHeight;
    const imageAspect = imageTexture.width / imageTexture.height;
    const aspect = canvasAspect / imageAspect;
    const aspectScale = aspect > 1 ? [1, aspect, 1] : [1 / aspect, 1, 1];

    const matrix = mat4.identity();
    mat4.scale(matrix, aspectScale, matrix);
    mat4.scale(matrix, [2, 2, 1], matrix);
    mat4.translate(matrix, [-0.5, -0.5, 1], matrix);

    // Set the uniform values in our JavaScript side Float32Array
    device.queue.writeBuffer(imageUniformBuffer, 0, matrix);

    const encoder = device.createCommandEncoder();
    const pass = encoder.beginRenderPass(renderPassDescriptor);
    pass.setPipeline(pipeline);
    pass.setBindGroup(0, imageBindGroup);
    pass.draw(6);
    pass.end();

    postProcess(encoder, renderTarget, canvasTexture);

    const commandBuffer = encoder.finish();
    device.queue.submit([commandBuffer]);
  }

  const observer = new ResizeObserver(entries => {
    for (const entry of entries) {
      const canvas = entry.target;
      const width = entry.contentBoxSize[0].inlineSize;
      const height = entry.contentBoxSize[0].blockSize;
      canvas.width = Math.max(1, Math.min(width, device.limits.maxTextureDimension2D));
      canvas.height = Math.max(1, Math.min(height, device.limits.maxTextureDimension2D));
    }
    render();
  });
  observer.observe(canvas);

  async function readImageFile(file) {
    const newImageTexture = await createTextureFromImage(device, URL.createObjectURL(file));
    imageTexture.destroy();
    imageTexture = newImageTexture;
    updateBindGroup();
    render();
  }

  dragAndDrop.setup({msg: 'Drop Image File here'});
  dragAndDrop.onDropFile(readImageFile);

  onPasteImage(readImageFile);
}

function fail(msg) {
  alert(msg);
}

main();
  </script>
</html>
